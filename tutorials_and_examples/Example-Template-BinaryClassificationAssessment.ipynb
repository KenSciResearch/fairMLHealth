{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Fairness Assessment Template - Example\n",
    "\n",
    "## About\n",
    "This example is intended as a simple illustration of the [Binary Classification Fairness Assessment Template](../templates/Template-BinaryClassificationAssessment.ipynb). It compares a Random Forest Classifier against fairness-aware alternative versions of that same classifier. For more information about the specific measures used, please see the [Measuring Fairness in Binary Classification Tutorial](../tutorials_and_examples/Tutorial-MeasuringFairnessInBinaryClassification.ipynb).\n",
    "\n",
    "For simplicity, only two fairness-aware algorithms are compared in this notebook. However, several other fairness-aware models were tested during development. For a peek at that process, see [Supplemental - Models for Binary Classification Example](../tutorials_and_examples/Supplemental-ModelsForBinaryClassificationExample.ipynb).\n",
    "\n",
    "## Example Contents\n",
    "\n",
    "[Part 0](#part0) - Data Loading and Baseline Model Setup\n",
    "\n",
    "[Part 1](#part1) - Measuring a Single (Baseline) Model\n",
    "\n",
    "[Part 2](#part2) - Fairness-Aware Models\n",
    "\n",
    "[Part 3](#part3) - Compare Several Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, HTML\n",
    "from fairmlhealth import model_comparison as fhmc, reports, stratified_reports, tutorial_helpers as helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Load (or Generate) Data and Models <a name=\"part0\"></a>\n",
    "\n",
    "Here you should load (or generate) your test dataset and models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIMIC-III\n",
    "\n",
    "This example uses a simple data subset from the [MIMIC-III clinical database](https://mimic.physionet.org/gettingstarted/access/) to predict the length of ICU stay (LOS) for a set of encounters. MIMIC-III is a freely available database, however all users must pass a quick human subjects certification course. For the example, LOS is the total intensive care unit (ICU) time for a given hospital admission in patients 65 and above. The raw LOS value is then converted to a binary value specifying whether an admission's length of stay is greater than the sample mean. \n",
    "\n",
    "Note that the code below will automatically unzip and format all necessary data for these experiments from a raw download of MIMIC-III data (saving the formatted data in the same MIMIC folder). If you would like to run this example on your own, [follow these steps to be granted access to MIMIC III](https://mimic.physionet.org/gettingstarted/access/) and download the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Subset\n",
    "\n",
    "Data are imported at the encounter level with all additional patient identification dropped. Boolean diagnosis and procedure features are categorized through the Clinical Classifications Software system ([HCUP](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp)). All features other than age are one-hot encoded and prefixed with their variable type (e.g. \"GENDER_\", \"ETHNICITY_\").  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_mimic_data_folder = \"[path to folder containing your MIMIC-III zip files]\"\n",
    "path_to_mimic_data_folder = \"~/data/MIMIC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data and keep a 10K observation subset to speed processing\n",
    "df = helpers.load_mimic3_example(path_to_mimic_data_folder) \n",
    "df = df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Subset to ages 65+\n",
    "df = df.loc[df['AGE'].ge(65), :]\n",
    "df.drop('GENDER_F', axis=1, inplace=True) # Redundant with GENDER_M\n",
    "\n",
    "\n",
    "# Generate a binary target flagging whether an observation's length_of_stay value is above or below the mean. \n",
    "mean_val = df['length_of_stay'].mean()\n",
    "df['long_los'] = df['length_of_stay'].apply(lambda x: 1 if x > mean_val else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Subset and Split Data\n",
    "X = df.loc[:, [c for c in df.columns \n",
    "                if c not in ['ADMIT_ID', 'length_of_stay', 'long_los']]]\n",
    "y = df.loc[:, ['long_los']]\n",
    "splits = train_test_split(X, y, stratify=y, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test=splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Baseline\n",
    "\n",
    "A Scikit-Learn Random Forest Classifier serves as our basis for comparison. Parameters were tuned using Scikit-Learn's GridSearch in the [Supplemental - Models for Binary Classification Example](../tutorials_and_examples/Supplemental-ModelsForBinaryClassificationExample.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Random Forest Prediction Scores: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " LOS <= mean       0.76      0.90      0.82       982\n",
      "  LOS > mean       0.77      0.55      0.64       614\n",
      "\n",
      "    accuracy                           0.76      1596\n",
      "   macro avg       0.76      0.72      0.73      1596\n",
      "weighted avg       0.76      0.76      0.75      1596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set model parameters (currently set as default values, but defined here to be explicit)\n",
    "rf_params = {'n_estimators': 1800, 'min_samples_split': 5, 'bootstrap': False}\n",
    "\n",
    "# Train Model\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "rf_model.fit(X_train, y_train.iloc[:, 0])\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# display performance \n",
    "print(\"\\n\", \"Random Forest Prediction Scores:\", \"\\n\", \n",
    "      classification_report(y_test, y_pred_rf, target_names=['LOS <= mean', 'LOS > mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Evaluate a Single (Baseline) Model <a name=\"part1\"></a>\n",
    "\n",
    "### Required Variables  \n",
    "\n",
    "- X_test = test data to be passed to the models to generate predictions.\n",
    "- y_test = target data array corresponding to X. \n",
    "- X_test['LANGUAGE_ENGL'] = protected attributes data corresponding to X, optionally also included in X. \n",
    "- rf_model = the trained model to be evaluated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_828e8_\" ><thead>    <tr>        <th class=\"blank\" ></th>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Value</th>    </tr>    <tr>        <th class=\"index_name level0\" >Metric</th>        <th class=\"index_name level1\" >Measure</th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_828e8_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"8\">Group Fairness</th>\n",
       "                        <th id=\"T_828e8_level1_row0\" class=\"row_heading level1 row0\" >Statistical Parity Difference</th>\n",
       "                        <td id=\"T_828e8_row0_col0\" class=\"data row0 col0\" >0.006400</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_828e8_level1_row1\" class=\"row_heading level1 row1\" >Disparate Impact Ratio</th>\n",
       "                        <td id=\"T_828e8_row1_col0\" class=\"data row1 col0\" >1.023500</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_828e8_level1_row2\" class=\"row_heading level1 row2\" >Equalized Odds Difference</th>\n",
       "                        <td id=\"T_828e8_row2_col0\" class=\"data row2 col0\" >0.014600</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_828e8_level1_row3\" class=\"row_heading level1 row3\" >Equalized Odds Ratio</th>\n",
       "                        <td id=\"T_828e8_row3_col0\" class=\"data row3 col0\" >0.952600</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_828e8_level1_row4\" class=\"row_heading level1 row4\" >Positive Predictive Parity Difference</th>\n",
       "                        <td id=\"T_828e8_row4_col0\" class=\"data row4 col0\" >0.019900</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_828e8_level1_row5\" class=\"row_heading level1 row5\" >Balanced Accuracy Difference</th>\n",
       "                        <td id=\"T_828e8_row5_col0\" class=\"data row5 col0\" >0.009800</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_828e8_level1_row6\" class=\"row_heading level1 row6\" >Balanced Accuracy Ratio</th>\n",
       "                        <td id=\"T_828e8_row6_col0\" class=\"data row6 col0\" >1.013700</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_828e8_level1_row7\" class=\"row_heading level1 row7\" >AUC Difference</th>\n",
       "                        <td id=\"T_828e8_row7_col0\" class=\"data row7 col0\" >0.010900</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_828e8_level0_row8\" class=\"row_heading level0 row8\" rowspan=\"2\">Individual Fairness</th>\n",
       "                        <th id=\"T_828e8_level1_row8\" class=\"row_heading level1 row8\" >Consistency Score</th>\n",
       "                        <td id=\"T_828e8_row8_col0\" class=\"data row8 col0\" >0.806400</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_828e8_level1_row9\" class=\"row_heading level1 row9\" >Between-Group Gen. Entropy Error</th>\n",
       "                        <td id=\"T_828e8_row9_col0\" class=\"data row9 col0\" >0.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_828e8_level0_row10\" class=\"row_heading level0 row10\" >Data Metrics</th>\n",
       "                        <th id=\"T_828e8_level1_row10\" class=\"row_heading level1 row10\" >Prevalence of Privileged Class (%)</th>\n",
       "                        <td id=\"T_828e8_row10_col0\" class=\"data row10 col0\" >38.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate comparison table (returned as a pandas dataframe)\n",
    "meas = fhmc.measure_model(X_test, y_test, X_test['LANGUAGE_ENGL'], rf_model)\n",
    "reports.flag(meas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Data Report\n",
    "\n",
    "FairMLHealth includes stratified reporting features to aid in identifying the source of unfairness or other bias. The data reporter evaluates basic statistics specific to each feature-value, in addition to relative statistics for the target value. Since the reporter can evaluate many features at once, it can be a useful option for identifying patterns of bias either alone or in concert with other (e.g., visual methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEATURE</th>\n",
       "      <th>FEATURE VALUE</th>\n",
       "      <th>N OBS</th>\n",
       "      <th>N MISSING</th>\n",
       "      <th>FEATURE ENTROPY</th>\n",
       "      <th>VALUE PREVALENCE</th>\n",
       "      <th>Y MAX</th>\n",
       "      <th>Y MEAN</th>\n",
       "      <th>Y MEDIAN</th>\n",
       "      <th>Y MIN</th>\n",
       "      <th>Y STDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL_FEATURES</td>\n",
       "      <td>ALL_VALUES</td>\n",
       "      <td>1596.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LANGUAGE_ENGL</td>\n",
       "      <td>0</td>\n",
       "      <td>776.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.4862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LANGUAGE_ENGL</td>\n",
       "      <td>1</td>\n",
       "      <td>820.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.5138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FEATURE FEATURE VALUE   N OBS  N MISSING  FEATURE ENTROPY  \\\n",
       "0   ALL_FEATURES    ALL_VALUES  1596.0          0              NaN   \n",
       "1  LANGUAGE_ENGL             0   776.0          0           0.9995   \n",
       "2  LANGUAGE_ENGL             1   820.0          0           0.9995   \n",
       "\n",
       "   VALUE PREVALENCE  Y MAX  Y MEAN  Y MEDIAN  Y MIN  Y STDV  \n",
       "0            1.0000    1.0  0.3847       0.0    0.0  0.4867  \n",
       "1            0.4862    1.0  0.3892       0.0    0.0  0.4879  \n",
       "2            0.5138    1.0  0.3805       0.0    0.0  0.4858  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratified_reports.data_report(X_test['LANGUAGE_ENGL'], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Performance Report\n",
    "\n",
    "The stratified classification_performance reporter evaluates model performance specific to each feature-value subset. If prediction probabilities (via the *predict_proba()* method) are available to the model, additional ROC_AUC and PR_AUC values will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEATURE</th>\n",
       "      <th>FEATURE VALUE</th>\n",
       "      <th>N OBS</th>\n",
       "      <th>TRUE MEAN</th>\n",
       "      <th>PRED MEAN</th>\n",
       "      <th>ACCURACY</th>\n",
       "      <th>FNR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PRECISION (PPV)</th>\n",
       "      <th>TNR</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL_FEATURES</td>\n",
       "      <td>ALL_VALUES</td>\n",
       "      <td>1596.0</td>\n",
       "      <td>0.3847</td>\n",
       "      <td>0.2751</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.4511</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>0.7677</td>\n",
       "      <td>0.8961</td>\n",
       "      <td>0.5489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LANGUAGE_ENGL</td>\n",
       "      <td>0</td>\n",
       "      <td>776.0</td>\n",
       "      <td>0.3892</td>\n",
       "      <td>0.2784</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>0.4437</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.8987</td>\n",
       "      <td>0.5563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LANGUAGE_ENGL</td>\n",
       "      <td>1</td>\n",
       "      <td>820.0</td>\n",
       "      <td>0.3805</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>0.7598</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.1063</td>\n",
       "      <td>0.7578</td>\n",
       "      <td>0.8937</td>\n",
       "      <td>0.5417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FEATURE FEATURE VALUE   N OBS  TRUE MEAN  PRED MEAN  ACCURACY  \\\n",
       "0   ALL_FEATURES    ALL_VALUES  1596.0     0.3847     0.2751    0.7625   \n",
       "1  LANGUAGE_ENGL             0   776.0     0.3892     0.2784    0.7655   \n",
       "2  LANGUAGE_ENGL             1   820.0     0.3805     0.2720    0.7598   \n",
       "\n",
       "      FNR     FPR  PRECISION (PPV)     TNR     TPR  \n",
       "0  0.4511  0.1039           0.7677  0.8961  0.5489  \n",
       "1  0.4437  0.1013           0.7778  0.8987  0.5563  \n",
       "2  0.4583  0.1063           0.7578  0.8937  0.5417  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratified_reports.classification_performance(X_test['LANGUAGE_ENGL'], y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Fairness Report\n",
    "\n",
    "The stratified classification_fairness reporter evaluates model fairness specific to each feature-value subset. It assumes each feature-value as the \"privileged\" group relative to all other possible values for the feature. To simplify the report, fairness measures have been simplified to their component parts. For example, measures of Equalized Odds can be determined by combining the True Positive Rate (TPR) Ratios & Differences with False Positive Rate (FPR) Ratios & Differences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEATURE</th>\n",
       "      <th>FEATURE VALUE</th>\n",
       "      <th>N OBS</th>\n",
       "      <th>FNR Diff</th>\n",
       "      <th>FNR Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>TNR Diff</th>\n",
       "      <th>TNR Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LANGUAGE_ENGL</td>\n",
       "      <td>0</td>\n",
       "      <td>776.0</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>1.0330</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.0497</td>\n",
       "      <td>-0.0199</td>\n",
       "      <td>-0.0199</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>-0.0146</td>\n",
       "      <td>0.9737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LANGUAGE_ENGL</td>\n",
       "      <td>1</td>\n",
       "      <td>820.0</td>\n",
       "      <td>-0.0146</td>\n",
       "      <td>0.9681</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.9526</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.0056</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>1.0270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FEATURE FEATURE VALUE  N OBS  FNR Diff  FNR Ratio  FPR Diff  \\\n",
       "0  LANGUAGE_ENGL             0  776.0    0.0146     1.0330     0.005   \n",
       "1  LANGUAGE_ENGL             1  820.0   -0.0146     0.9681    -0.005   \n",
       "\n",
       "   FPR Ratio  PPV Diff  PPV Ratio  TNR Diff  TNR Ratio  TPR Diff  TPR Ratio  \n",
       "0     1.0497   -0.0199    -0.0199    -0.005     0.9944   -0.0146     0.9737  \n",
       "1     0.9526    0.0199     0.0199     0.005     1.0056    0.0146     1.0270  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratified_reports.classification_fairness(X_test['LANGUAGE_ENGL'], y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Generate Fairness-Aware Models <a name=\"part2\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairlearn Models\n",
    "\n",
    "The [Fairlearn](https://fairlearn.github.io/) package includes three [mitigation algorithms](https://fairlearn.github.io/user_guide/mitigation.html) designed to increase the fairness of an existing model relative to one of two user-specified fairness metrics. Both algorithms and metrics are listed in the cell below.\n",
    "\n",
    "For more information about the specifics of these fairness metrics, see [Part 5 of the Measuring Fairness in Binary Classification Tutorial](../tutorials_and_examples/Tutorial-MeasuringFairnessInBinaryClassification.ipynb#part5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mitigation Algorithms\n",
    "from fairlearn.reductions import GridSearch, ExponentiatedGradient\n",
    "\n",
    "# Fairness Measures\n",
    "from fairlearn.reductions import EqualizedOdds, DemographicParity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair ExponentiatedGradient\n",
    "\n",
    "Fairlearn's ExponentiatedGradient is a wrapper that runs a constrained optimization using the Exponentiated Gradient approach on a binary classification model. It treats the prediction as a sequence of cost-sensitive classification problems, returning the solution with the smallest error (constrained by the metric of choice). This approach has been demonstrated to have minimal effect on model performance by some measures. [[Agarwal2018]](#Agarwal2018)\n",
    "\n",
    "This approach is applicable to sensitive attributes that are either categorical or binary/Boolean. It can be used for classification problems only.\n",
    "\n",
    "Note: solutions are not guaranteed for this approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for consistent results with Fairlearn's ExponentiatedGradient\n",
    "np.random.seed(36)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fair ExponentiatedGradient Using Demographic Parity as Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eg_rfDP_model = ExponentiatedGradient(RandomForestClassifier(**rf_params), \n",
    "                                      constraints=DemographicParity()) \n",
    "eg_rfDP_model.fit(X_train, y_train,\n",
    "                  sensitive_features=X_train['LANGUAGE_ENGL'])\n",
    "y_pred_eg_rfDP = eg_rfDP_model.predict(X_test)\n",
    "\n",
    "# display performance \n",
    "print(\"\\n\", \"Prediction Scores:\", \"\\n\", \n",
    "      classification_report(y_test, y_pred_eg_rfDP, \n",
    "       target_names=['LOS <= mean', 'LOS > mean']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fair ExponentiatedGradient Using Equalized Odds as Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eg_rfEO_model = ExponentiatedGradient(RandomForestClassifier(**rf_params), \n",
    "                                      constraints=EqualizedOdds())  \n",
    "eg_rfEO_model.fit(X_train, y_train, \n",
    "                  sensitive_features=X_train['LANGUAGE_ENGL'])\n",
    "y_pred_eg_rfEO = eg_rfEO_model.predict(X_test)\n",
    "\n",
    "# display performance \n",
    "print(\"\\n\", \"Prediction Scores:\", \"\\n\", \n",
    "      classification_report(y_test, y_pred_eg_rfEO, \n",
    "       target_names=['LOS <= mean', 'LOS > mean']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair GridSearch\n",
    "\n",
    "Fairlearn's GridSearch is a wrapper that runs a constrained optimization using the Grid Search approach  on a binary classification or a regression model. It treats the prediction as a sequence of cost-sensitive classification problems, returning the solution with the smallest error (constrained by the metric of choice). This approach has been demonstrated to have minimal effect on model performance by some measures [[Agarwal2018]](#Agarwal2018).\n",
    "\n",
    "This approach is applicable to sensitive attributes that are binary/Boolean only. It can be used for either binary classification or regression problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fair GridSearch Using Equalized Odds as Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train GridSearch\n",
    "gs_rfEO_model = GridSearch(RandomForestClassifier(**rf_params),\n",
    "                           constraints=EqualizedOdds(),\n",
    "                           grid_size=45)\n",
    "\n",
    "gs_rfEO_model.fit(X_train, y_train, \n",
    "                  sensitive_features = X_train['LANGUAGE_ENGL'])\n",
    "y_pred_gs_rfEO = gs_rfEO_model.predict(X_test)\n",
    "\n",
    "# display performance \n",
    "print(\"\\n\", \"Prediction Scores:\", \"\\n\", \n",
    "      classification_report(y_test, y_pred_gs_rfEO, \n",
    "       target_names=['LOS <= mean', 'LOS > mean']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fair GridSearch Using Demographic Parity as Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train GridSearch\n",
    "gs_rfDP_model = GridSearch(RandomForestClassifier(**rf_params),\n",
    "                           constraints=DemographicParity(),\n",
    "                           grid_size=45)\n",
    "\n",
    "gs_rfDP_model.fit(X_train, y_train, \n",
    "                  sensitive_features=X_train['LANGUAGE_ENGL'])\n",
    "y_pred_gs_rfDP = gs_rfDP_model.predict(X_test)\n",
    "\n",
    "# display performance \n",
    "print(\"\\n\", \"Prediction Scores:\", \"\\n\", \n",
    "      classification_report(y_test, y_pred_gs_rfDP, \n",
    "       target_names=['LOS <= mean', 'LOS > mean']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Compare Several Models <a name=\"part3\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Required Variables  \n",
    "\n",
    "* X_data (NumPy array or similar pandas object): Test data to be passed to the models to generate predictions (or list/dict of data for each model if model inputs differ). It's recommended that these be separate data from those used to train the model.\n",
    "\n",
    "* y_data (NumPy array or similar pandas object): Target data array corresponding to X (or list/dict of labels for each model if labels differ). It is recommended that the target is not present in the test data.\n",
    "\n",
    "* pa_data (NumPy array or similar pandas object): Protected attributes corresponding to X, optionally also included in X (or list/dict of data for each model if attributes differ). Note that values must currently be binary- or Boolean-type.\n",
    "\n",
    "* models (list or dict-like): The set of trained models to be evaluated (list or dict). Note that the dictionary keys are assumed as model names. If a list-like object is passed, the function will set model names relative to their index (i.e. \"model_0\", \"model_1\", etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = X_test\n",
    "y = y_test\n",
    "protected_attr = X_test['LANGUAGE_ENGL']\n",
    "models = {'rf_model': rf_model,\n",
    "         'gs_rfEO_model': gs_rfEO_model, 'gs_rfDP_model': gs_rfDP_model,\n",
    "         'eg_rfEO_model': eg_rfEO_model, 'eg_rfDP_model': eg_rfDP_model}\n",
    "display(\"Models being compared in this example:\", list(models.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with the FairMLHealth Tool\n",
    "\n",
    "The FairMLHealth model comparison tool generates a table of fairness measures that can be used to quickly compare the fairness-performance tradeoff for a set of fairness-aware models. \n",
    "\n",
    "Note that there is some additional formatting added to the cell below simply to add highlighting for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison table (returned as a pandas dataframe)\n",
    "comparison = fhmc.compare_models(X, y, protected_attr, models)\n",
    "\n",
    "# Here we determine the indices for equal odds measures so that we can highlight according\n",
    "#    to those indices later\n",
    "idx = pd.IndexSlice\n",
    "eotag = idx[:, ['Equal Opportunity Difference', 'Equalized Odds Difference',\n",
    "                 'Equalized Odds Ratio']\n",
    "            ]\n",
    "equal_odds = comparison.loc[eotag, :].index\n",
    "\n",
    "# Here we return the flagged table as a pandas styler so we can also highlight \n",
    "#       measures of Equal Odds\n",
    "flagged = reports.flag(comparison, as_styler=True)\n",
    "flagged.apply(lambda x: ['background-color:' + \"#DED8F9\" \n",
    "                          if x.name in equal_odds else '' for i in x]\n",
    "                , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<a name=\"Agarwal2018\"></a>\n",
    "Agarwal, A., Beygelzimer, A., Dud√≠k, M., Langford, J., & Wallach, H. (2018). A reductions approach to fair classification. In International Conference on Machine Learning (pp. 60-69). PMLR. Available through [arXiv preprint:1803.02453](https://arxiv.org/pdf/1803.02453.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
