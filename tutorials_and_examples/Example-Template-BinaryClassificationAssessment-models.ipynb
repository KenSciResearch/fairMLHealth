{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models -  Binary Classification Fairness Assesment Template - Example\n",
    "\n",
    "## About\n",
    "This example is intended as a simple illustration for the use of the [Binary Classification Fairness Assessment Template](../templates/Template-BinaryClassificationAssessment.ipynb). It compares a Random Forest Classifier against fairness-aware alternative versions of that same classifier. For more information about the specific measures used, please see the [Measuring Fairness in Binary Classification Tutorial](../tutorials_and_examples/Tutorial-MeasuringFairnessInBinaryClassification.ipynb).\n",
    "\n",
    "In the interest of simplicity, only two fairness-aware algorithms are compared in this notebook. However, several other fairness-aware models were tested in during development. For a peek at that process, see [Supplemental - Models for Binary Classification Example](../tutorials_and_examples/Supplemental-ModelsForBinaryClassificationExample.ipynb).\n",
    "\n",
    "## Example Contents\n",
    "\n",
    "[Part 1](#part1) - Data Loading and Model Setup\n",
    "\n",
    "[Part 2](#part2) - Fairness-Aware Models\n",
    "\n",
    "[Part 3](#part3) - Model Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from fairmlhealth import reports, tutorial_helpers as helpers, model_comparison as fhmc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pointers to make this example more colorful\n",
    "ks_magenta = '#d00095'\n",
    "ks_magenta_lt = '#ff05b8'\n",
    "ks_purple = '#947fed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Load Data and Generate Baseline Model <a name=\"part1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIMIC-III\n",
    "\n",
    "This example uses a data subset from the [MIMIC-III clinical database](https://mimic.physionet.org/gettingstarted/access/) to predict \"length of stay\" (LOS) value. For this example, LOS is total ICU time for a given hospital admission in patients 65 and above. The raw LOS value is then converted to a binary value specifying whether an admission's length of stay is greater than the sample mean. \n",
    "\n",
    "Note that the code below will automatically unzip and format all necessary data for these experiments from a raw download of MIMIC-III data (saving the formatted data in the same MIMIC folder). MIMIC-III is a freely available database, however all users must pass a quick human subjects certification course. If you would like to run this example on your own, [follow these steps to be granted access to MIMIC III](https://mimic.physionet.org/gettingstarted/access/) and download the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Subset\n",
    "\n",
    "Data are imported at the encounter level with all additional patient identification dropped. Boolean diagnosis and procedure features are categorized through the Clinical Classifications Software system ([HCUP](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp)). All features other than age are one-hot encoded and prefixed with their variable type (e.g. \"GENDER_\", \"ETHNICITY_\").  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_mimic_data_folder = \"[path to folder containing your MIMIC-III zip files]\"\n",
    "path_to_mimic_data_folder = \"~/data/MIMIC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This data subset has 22434 total observations and 648 input features \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw Feature</th>\n",
       "      <th>Category Count (Encoded Features)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DIAGNOSIS</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ETHNICITY</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GENDER</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INSURANCE</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LANGUAGE</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MARRIED</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PROCEDURE</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RELIGION</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Raw Feature  Category Count (Encoded Features)\n",
       "0         AGE                                  1\n",
       "1   DIAGNOSIS                                282\n",
       "2   ETHNICITY                                 41\n",
       "3      GENDER                                  1\n",
       "4   INSURANCE                                  5\n",
       "5    LANGUAGE                                 69\n",
       "6     MARRIED                                  7\n",
       "7   PROCEDURE                                222\n",
       "8    RELIGION                                 20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow0_col1,#T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow1_col1{\n",
       "            background-color:  #ff05b8;\n",
       "        }</style><table id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4b\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >count</th>        <th class=\"col_heading level0 col1\" >mean</th>        <th class=\"col_heading level0 col2\" >std</th>        <th class=\"col_heading level0 col3\" >min</th>        <th class=\"col_heading level0 col4\" >25%</th>        <th class=\"col_heading level0 col5\" >50%</th>        <th class=\"col_heading level0 col6\" >75%</th>        <th class=\"col_heading level0 col7\" >max</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4blevel0_row0\" class=\"row_heading level0 row0\" >length_of_stay</th>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow0_col0\" class=\"data row0 col0\" >22434.000000</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow0_col1\" class=\"data row0 col1\" >9.115200</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow0_col2\" class=\"data row0 col2\" >6.208700</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow0_col3\" class=\"data row0 col3\" >0.004200</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow0_col4\" class=\"data row0 col4\" >4.735200</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow0_col5\" class=\"data row0 col5\" >7.579900</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow0_col6\" class=\"data row0 col6\" >12.017700</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow0_col7\" class=\"data row0 col7\" >29.988900</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4blevel0_row1\" class=\"row_heading level0 row1\" >long_los</th>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow1_col0\" class=\"data row1 col0\" >22434.000000</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow1_col1\" class=\"data row1 col1\" >0.388000</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow1_col2\" class=\"data row1 col2\" >0.487300</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow1_col5\" class=\"data row1 col5\" >0.000000</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow1_col6\" class=\"data row1 col6\" >1.000000</td>\n",
       "                        <td id=\"T_1808ccf4_13f0_11eb_90f0_f0189849bf4brow1_col7\" class=\"data row1 col7\" >1.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f86400a7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data and subset to ages 65+\n",
    "df = helpers.load_mimic3_example(path_to_mimic_data_folder) \n",
    "df = df.loc[df['AGE'].ge(65), :]\n",
    "df.drop('GENDER_F', axis=1, inplace=True) # Redundant with GENDER_M\n",
    "\n",
    "# Show variable count\n",
    "helpers.print_feature_table(df)\n",
    "display(Markdown('---'))\n",
    "\n",
    "# Generate a binary target flagging whether an observation's length_of_stay value is above or below the mean. \n",
    "mean_val = df['length_of_stay'].mean()\n",
    "df['long_los'] = df['length_of_stay'].apply(lambda x: 1 if x > mean_val else 0)\n",
    "los_tbl = df[['length_of_stay', 'long_los']].describe().transpose().round(4)\n",
    "tbl_style = los_tbl.style.applymap(helpers.highlight_col, \n",
    "                                    subset=pd.IndexSlice[:, 'mean'],\n",
    "                                    color=ks_magenta_lt\n",
    "                                  )\n",
    "display(tbl_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Subset and Split Data\n",
    "X = df.loc[:, [c for c in df.columns \n",
    "                if c not in ['ADMIT_ID','length_of_stay', 'long_los']]]\n",
    "y = df.loc[:, ['long_los']]\n",
    "splits = train_test_split(X, y, stratify=y, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Baseline\n",
    "\n",
    "A Scikit-Learn Random Forest Classifier serves as our basis for comparison. Parameters were tuned using Scikit-Learn's GridSearch in the [Supplemental - Models for Binary Classification Example](../tutorials_and_examples/Supplemental-ModelsForBinaryClassificationExample.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Random Forest Prediction Scores: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " LOS <= mean       0.77      0.90      0.83      4531\n",
      "  LOS > mean       0.78      0.58      0.66      2873\n",
      "\n",
      "    accuracy                           0.77      7404\n",
      "   macro avg       0.77      0.74      0.75      7404\n",
      "weighted avg       0.77      0.77      0.76      7404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set model parameters (currently set as default values, but defined here to be explicit)\n",
    "rf_params = {'n_estimators': 1800, 'min_samples_split': 5, 'bootstrap': False}\n",
    "\n",
    "# Train Model\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "rf_model.fit(X_train, y_train.iloc[:, 0])\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# display performance \n",
    "print(\"\\n\", \"Random Forest Prediction Scores:\", \"\\n\", \n",
    "      classification_report(y_test, y_pred_rf, \n",
    "                            target_names=['LOS <= mean', 'LOS > mean']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Fairness-Aware Models <a name=\"part2\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FairLearn Models\n",
    "\n",
    "The [FairLearn](https://fairlearn.github.io/) package includes three [mitigation algorithms](https://fairlearn.github.io/user_guide/mitigation.html) designed to increase the fairness of an existing model relative to one of two user-specified fairness metrics. Both algorithms and metrics are listed in the cell below.\n",
    "\n",
    "For more information about the specifics of these fairness metrics, see also [Part 5 of the Measuring Fairness in Binary Classification Tutorial](../tutorials_and_examples/Tutorial-MeasuringFairnessInBinaryClassification.ipynb#part5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mitigation Algorithms\n",
    "from fairlearn.reductions import GridSearch, ExponentiatedGradient\n",
    "\n",
    "# Fairness Measures\n",
    "from fairlearn.reductions import EqualizedOdds, DemographicParity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair ExponentiatedGradient\n",
    "\n",
    "FairLearn's ExponentiatedGradient is a wrapper that runs a constrained optimization using the Exponentiated Gradient approach on a binary classification model. It treats the prediction as a sequence of cost-sensitive classification problems, returning the solution with the smallest error (constrained by the metric of choice). This approach has been demonstrated to have minimal effect on model performance by some measures. [Agarwal2018](#Agarwal2018)\n",
    "\n",
    "This approach is applicable to sensitive attributes that are either categorical or binary/boolean. It can be used for classification problems only.\n",
    "\n",
    "Note: solutions are not guaranteed for this approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for consistent results with FairLearn's ExponentiatedGradient\n",
    "np.random.seed(36)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fair ExponentiatedGradient Using Demographic Parity as Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Prediction Scores: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " LOS <= mean       0.74      0.89      0.81      4531\n",
      "  LOS > mean       0.74      0.52      0.61      2873\n",
      "\n",
      "    accuracy                           0.74      7404\n",
      "   macro avg       0.74      0.70      0.71      7404\n",
      "weighted avg       0.74      0.74      0.73      7404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eg_rfDP_model = ExponentiatedGradient(RandomForestClassifier(**rf_params), \n",
    "                                      constraints=DemographicParity()) \n",
    "eg_rfDP_model.fit(X_train, y_train,\n",
    "                  sensitive_features=X_train['LANGUAGE_ENGL'])\n",
    "y_pred_eg_rfDP = eg_rfDP_model.predict(X_test)\n",
    "\n",
    "# display performance \n",
    "print(\"\\n\", \"Prediction Scores:\", \"\\n\", \n",
    "      classification_report(y_test, y_pred_eg_rfDP, \n",
    "                            target_names=['LOS <= mean', 'LOS > mean']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fair ExponentiatedGradient Using Equalized Odds as Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Prediction Scores: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " LOS <= mean       0.77      0.89      0.83      4531\n",
      "  LOS > mean       0.77      0.58      0.66      2873\n",
      "\n",
      "    accuracy                           0.77      7404\n",
      "   macro avg       0.77      0.74      0.74      7404\n",
      "weighted avg       0.77      0.77      0.76      7404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eg_rfEO_model = ExponentiatedGradient(RandomForestClassifier(**rf_params), \n",
    "                                      constraints=EqualizedOdds())  \n",
    "eg_rfEO_model.fit(X_train, y_train, \n",
    "                  sensitive_features=X_train['LANGUAGE_ENGL'])\n",
    "y_pred_eg_rfEO = eg_rfEO_model.predict(X_test)\n",
    "\n",
    "# display performance \n",
    "print(\"\\n\", \"Prediction Scores:\", \"\\n\", \n",
    "      classification_report(y_test, y_pred_eg_rfEO, \n",
    "                            target_names=['LOS <= mean', 'LOS > mean']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair GridSearch\n",
    "\n",
    "FairLearn's GridSearch is a wrapper that runs a constrained optimization using the Grid Search approach  on a binary classification or a regression model. It treats the prediction as a sequence of cost-sensitive classification problems, returning the solution with the smallest error (constrained by the metric of choice). This approach has been demonstrated to have minimal effect on model performance by some measures. [[Agarwal2018]](#Agarwal2018)\n",
    "\n",
    "This approach is applicable to sensitive attributes that are binary/boolean only. It can be used for either binary classification or regression problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fair GridSearch Using Equalized Odds as Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Prediction Scores: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " LOS <= mean       0.77      0.89      0.83      4531\n",
      "  LOS > mean       0.77      0.58      0.66      2873\n",
      "\n",
      "    accuracy                           0.77      7404\n",
      "   macro avg       0.77      0.74      0.74      7404\n",
      "weighted avg       0.77      0.77      0.76      7404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train GridSearch\n",
    "gs_rfEO_model = GridSearch(RandomForestClassifier(**rf_params),\n",
    "                           constraints=EqualizedOdds(),\n",
    "                           grid_size=45)\n",
    "\n",
    "gs_rfEO_model.fit(X_train, y_train, \n",
    "                  sensitive_features=X_train['LANGUAGE_ENGL'])\n",
    "y_pred_gs_rfEO = gs_rfEO_model.predict(X_test)\n",
    "\n",
    "# display performance \n",
    "print(\"\\n\", \"Prediction Scores:\", \"\\n\", \n",
    "      classification_report(y_test, y_pred_gs_rfEO, \n",
    "                            target_names=['LOS <= mean', 'LOS > mean']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fair GridSearch Using Demographic Parity as Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Prediction Scores: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " LOS <= mean       0.75      0.89      0.81      4531\n",
      "  LOS > mean       0.74      0.53      0.62      2873\n",
      "\n",
      "    accuracy                           0.75      7404\n",
      "   macro avg       0.75      0.71      0.71      7404\n",
      "weighted avg       0.75      0.75      0.73      7404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train GridSearch\n",
    "gs_rfDP_model = GridSearch(RandomForestClassifier(**rf_params),\n",
    "                           constraints=DemographicParity(),\n",
    "                           grid_size=45)\n",
    "\n",
    "gs_rfDP_model.fit(X_train, y_train, \n",
    "                  sensitive_features=X_train['LANGUAGE_ENGL'])\n",
    "y_pred_gs_rfDP = gs_rfDP_model.predict(X_test)\n",
    "\n",
    "# display performance \n",
    "print(\"\\n\", \"Prediction Scores:\", \"\\n\", \n",
    "      classification_report(y_test, y_pred_gs_rfDP, \n",
    "                            target_names=['LOS <= mean', 'LOS > mean']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Model Comparison <a name=\"part3\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Required Variables  \n",
    "\n",
    "* X (numpy array or similar pandas object): test data to be passed to the models to generate predictions. It's recommended that these be separate data from those used to train the model.\n",
    "\n",
    "* y (numpy array or similar pandas object): target data array corresponding to X. It is recommended that the target is not present in the test_data.\n",
    "\n",
    "* models (list or dict-like): the set of trained models to be evaluated. Note that the dictionary keys are assumed as model names. If a list-like object is passed, the function will set model names relative to their index (i.e. \"model_0\", \"model_1\", etc.)\n",
    "\n",
    "* protected_attr (numpy array or similar pandas object): protected attributes correspoinding to X, optionally also included in X. Note that values must currently be binary- or boolean-type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models being compared in this example: ['rf_model', 'gs_rfEO_model', 'gs_rfDP_model', 'eg_rfEO_model', 'eg_rfDP_model']\n"
     ]
    }
   ],
   "source": [
    "X = X_test\n",
    "y = y_test\n",
    "protected_attr = X_test['LANGUAGE_ENGL']\n",
    "models = {'rf_model': rf_model,\n",
    "         'gs_rfEO_model': gs_rfEO_model, 'gs_rfDP_model': gs_rfDP_model,\n",
    "         'eg_rfEO_model': eg_rfEO_model, 'eg_rfDP_model': eg_rfDP_model}\n",
    "print(\"Models being compared in this example:\", list(models.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with the FairMLHealth Tool\n",
    "\n",
    "The FairMLHealth model comparison tool generates a table of fairness measures that can be used to quickly compare the fairness-performance tradeoff for a set of fairness-aware models. \n",
    "\n",
    "Note that there is some additional formatting added to the cell below simply to add highlighting for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/christineallen/data/fairMLHealth/fairml_binaryExampleModels_data.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, joblib\n",
    "\n",
    "output_file = os.path.expanduser(\"~/data/fairMLHealth/fairml_binaryExampleModels\")\n",
    "\n",
    "model_file = output_file + \"_model.joblib\"\n",
    "data_file = output_file + \"_data.joblib\"\n",
    "\n",
    "\n",
    "data = {'X':X, 'y':y, 'protected_attr':protected_attr}\n",
    "joblib.dump(data, data_file, compress=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving models...\n",
      "\t rf_model\n",
      "\t gs_rfEO_model\n",
      "\t gs_rfDP_model\n",
      "\t eg_rfEO_model\n",
      "Cannot save model eg_rfEO_model. Can't pickle local object '_Lagrangian.best_h.<locals>.h'\n",
      "\t eg_rfDP_model\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "\n",
    "\n",
    "class ValidationError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "if os.path.exists(model_file):\n",
    "    raise ValidationError(f\"File already exists: {model_file}.\")\n",
    "else:\n",
    "    print(\"saving models...\")\n",
    "with open(model_file, 'wb') as file:\n",
    "    for name, model in models.items():\n",
    "        setattr(model, 'fmlh_model_name', name)\n",
    "        print(\"\\t\", name)\n",
    "        try:\n",
    "            joblib.dump(model, file, compress=3)\n",
    "        except BaseException as e:\n",
    "            #logger.error(\"Cannot save models. \" + str(e)) \n",
    "            print(f\"Cannot save model {name}.\", str(e))\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_file, \"rb\") as f:\n",
    "    test = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "load_models = {}\n",
    "with open(model_file, \"rb\") as f:\n",
    "    while True:\n",
    "        try:\n",
    "            new_model = joblib.load(f)\n",
    "            load_models[new_model.model_name] = new_model\n",
    "        except EOFError:\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<a name=\"Agarwal2018\"></a>\n",
    "Agarwal, A., Beygelzimer, A., Dud√≠k, M., Langford, J., & Wallach, H. (2018). A reductions approach to fair classification. [rXiv preprint arXiv:1803.02453](https://arxiv.org/pdf/1803.02453.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
