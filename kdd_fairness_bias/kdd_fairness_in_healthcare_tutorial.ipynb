{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial of Fairness Metrics for Healthcare\n",
    "\n",
    "### Overview\n",
    "This tutorial introduces methods and libraries for measuring fairness and bias in machine learning models as as they relate to problems in healthcare. After providing some background, it will generate a simple baseline model predicting Length of Stay (LOS) using data from the [MIMIC-III database](https://mimic.physionet.org/gettingstarted/access/). It will then use variations of that model to demonstrate common measures of \"fairness\" using [AIF360](http://aif360.mybluemix.net/), a prominent library for this purpose, before comparing AIF360 to another prominent library, [FairLearn](https://fairlearn.github.io/).\n",
    "  \n",
    "### Tutorial Contents\n",
    "[Part 0:] Background\n",
    "\n",
    "[Part 1:](#part1) Model Setup\n",
    "\n",
    "[Part 2:](#part2) Metrics of Fairness in AIF360\n",
    "\n",
    "[Part 3:](#part3) Comparing Against a Second Model - Evaluating Unawarenes\n",
    "\n",
    "[Part 4:](#part4) Testing Other Sensitive Attributes\n",
    "\n",
    "[Part 5:](#part5) Comparison to FairLearn\n",
    "\n",
    "### Requirements\n",
    "This tutorial assumes basic knowledge of machine learning implementation in Python. Before starting, please install [AIF360](http://aif360.mybluemix.net/) and [FairLearn](https://fairlearn.github.io/). Also, ensure that you have installed the Pandas, Numpy, Scikit, and XGBOOST libraries.\n",
    "\n",
    "The tutorial also uses data from the MIMIC III Critical Care database, a freely accessible source of Electronic Health Records from Beth Israel Deaconess Medical Center in Boston. To download the MIMIC III data, please use this link: [Access to MIMIC III](https://mimic.physionet.org/gettingstarted/access/). Please save the data with the default directory name (\"MIMIC\"). No further action is required beyond remembering the download location: you do not need to unzip any files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Background \n",
    "SECTIONS TO BE INCLUDED:\n",
    "* what is fairness\n",
    "* metrics for fairness\n",
    "* list of measures that will be included in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Model Setup <a class=\"anchor\" id=\"part1\"></a>\n",
    "\n",
    "This section introduces and loads the data subset that will be used in this tutorial. Then it generates a simple baseline model to be used throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Prediction Libraries\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# Metrics\n",
    "import sklearn.metrics as sk_metrics\n",
    "from aif360.sklearn.metrics import *\n",
    "from fairlearn.metrics import (\n",
    "    selection_rate as fairlearn_selection_rate, \n",
    "    demographic_parity_difference, demographic_parity_ratio,\n",
    "    balanced_accuracy_score_group_summary, roc_auc_score_group_summary,\n",
    "    equalized_odds_difference, equalized_odds_ratio,\n",
    "    difference_from_summary)\n",
    "\n",
    "# Helpers from local folder\n",
    "import tutorial_helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC III Data Subset\n",
    "As mentioned aboce, the MIMIC-III data download contains a folder of zipped_files. The tutorial code will automatically unzip and format all necessary data for these experiments, saving the formatted data in the MIMIC folder. Simply enter the correct path of the MIMIC folder in the following cell to enable this feature. Your path should end with the directory \"MIMIC\".\n",
    "\n",
    "Example: path_to_mimic_data_folder = \"~/data/MIMIC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_mimic_data_folder = \"[path to your downloaded data folder]\"\n",
    "path_to_mimic_data_folder = \"~/data/MIMIC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Subset\n",
    "Example models in this notebook use data from all years of the MIMIC-III dataset for patients aged 65 and older. Data are imported at the encounter level with all additional patient identification dropped. All models include an \"AGE\" feature, simplified to 5-year bins, as well as boolean diagnosis and procedure features categorized through the Clinical Classifications Software system ([HCUP](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp)). All features other than age are one-hot encoded and prefixed with their variable type (e.g. \"GENDER_\", \"ETHNICITY_\").  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADMIT_ID</th>\n",
       "      <th>AGE</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>GENDER_M</th>\n",
       "      <th>ETHNICITY_AMERICAN INDIAN/ALASKA NATIVE</th>\n",
       "      <th>ETHNICITY_AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE</th>\n",
       "      <th>ETHNICITY_ASIAN</th>\n",
       "      <th>ETHNICITY_ASIAN - ASIAN INDIAN</th>\n",
       "      <th>ETHNICITY_ASIAN - CAMBODIAN</th>\n",
       "      <th>ETHNICITY_ASIAN - CHINESE</th>\n",
       "      <th>...</th>\n",
       "      <th>PROCEDURE_CCS_221</th>\n",
       "      <th>PROCEDURE_CCS_222</th>\n",
       "      <th>PROCEDURE_CCS_223</th>\n",
       "      <th>PROCEDURE_CCS_224</th>\n",
       "      <th>PROCEDURE_CCS_225</th>\n",
       "      <th>PROCEDURE_CCS_226</th>\n",
       "      <th>PROCEDURE_CCS_227</th>\n",
       "      <th>PROCEDURE_CCS_228</th>\n",
       "      <th>PROCEDURE_CCS_229</th>\n",
       "      <th>PROCEDURE_CCS_231</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232464</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.144444</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219372</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5.496528</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191470</td>\n",
       "      <td>75.0</td>\n",
       "      <td>6.768056</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>264810</td>\n",
       "      <td>70.0</td>\n",
       "      <td>6.988889</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>229718</td>\n",
       "      <td>75.0</td>\n",
       "      <td>5.364583</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ADMIT_ID   AGE  length_of_stay  GENDER_M  \\\n",
       "0    232464  65.0        1.144444         0   \n",
       "1    219372  70.0        5.496528         1   \n",
       "2    191470  75.0        6.768056         1   \n",
       "5    264810  70.0        6.988889         1   \n",
       "7    229718  75.0        5.364583         1   \n",
       "\n",
       "   ETHNICITY_AMERICAN INDIAN/ALASKA NATIVE  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "5                                        0   \n",
       "7                                        0   \n",
       "\n",
       "   ETHNICITY_AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE  \\\n",
       "0                                                  0                    \n",
       "1                                                  0                    \n",
       "2                                                  0                    \n",
       "5                                                  0                    \n",
       "7                                                  0                    \n",
       "\n",
       "   ETHNICITY_ASIAN  ETHNICITY_ASIAN - ASIAN INDIAN  \\\n",
       "0                0                               0   \n",
       "1                0                               0   \n",
       "2                0                               0   \n",
       "5                0                               0   \n",
       "7                0                               0   \n",
       "\n",
       "   ETHNICITY_ASIAN - CAMBODIAN  ETHNICITY_ASIAN - CHINESE  ...  \\\n",
       "0                            0                          0  ...   \n",
       "1                            0                          0  ...   \n",
       "2                            0                          0  ...   \n",
       "5                            0                          0  ...   \n",
       "7                            0                          0  ...   \n",
       "\n",
       "   PROCEDURE_CCS_221  PROCEDURE_CCS_222  PROCEDURE_CCS_223  PROCEDURE_CCS_224  \\\n",
       "0                  0                  0                  0                  0   \n",
       "1                  0                  1                  0                  0   \n",
       "2                  0                  0                  0                  0   \n",
       "5                  0                  0                  0                  0   \n",
       "7                  0                  1                  0                  0   \n",
       "\n",
       "   PROCEDURE_CCS_225  PROCEDURE_CCS_226  PROCEDURE_CCS_227  PROCEDURE_CCS_228  \\\n",
       "0                  0                  0                  0                  0   \n",
       "1                  0                  0                  0                  0   \n",
       "2                  0                  0                  0                  0   \n",
       "5                  0                  0                  1                  0   \n",
       "7                  0                  0                  1                  0   \n",
       "\n",
       "   PROCEDURE_CCS_229  PROCEDURE_CCS_231  \n",
       "0                  0                  0  \n",
       "1                  0                  0  \n",
       "2                  0                  0  \n",
       "5                  0                  0  \n",
       "7                  0                  0  \n",
       "\n",
       "[5 rows x 650 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tutorial_helpers.load_example_data(path_to_mimic_data_folder) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Length of Stay Model\n",
    "Example models in this tutorial predict the length of time spent in the ICU, a.k.a. the \"Length of Stay\" (LOS). The baseline model will use only the patient's age, their diagnosis, and the use of medical procedures during their stay to predict this value. \n",
    "\n",
    "Two target variables will be used in the following experiments: 'length_of_stay' and 'los_binary'. For this dataset, length_of_stay is, of course, the true value of the length of the patient's stay in days. The los_binary variable is a binary variable indicating whether the admission resulted in a length of stay either < or >= the mean. We will generate variable below, and then generate our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>los_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22434.0000</td>\n",
       "      <td>22434.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.1152</td>\n",
       "      <td>0.3880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.2087</td>\n",
       "      <td>0.4873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.7352</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.5799</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.0177</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29.9889</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       length_of_stay  los_binary\n",
       "count      22434.0000  22434.0000\n",
       "mean           9.1152      0.3880\n",
       "std            6.2087      0.4873\n",
       "min            0.0042      0.0000\n",
       "25%            4.7352      0.0000\n",
       "50%            7.5799      0.0000\n",
       "75%           12.0177      1.0000\n",
       "max           29.9889      1.0000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a binary target flagging whether an observation's length_of_stay value is above or below the mean. \n",
    "mean_val=df['length_of_stay'].mean()\n",
    "df['los_binary'] = df['length_of_stay'].apply(lambda x: 0 if x < mean_val else 1)\n",
    "df[['length_of_stay', 'los_binary']].describe().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Baseline ROC_AUC Score: 0.8247621989647158\n"
     ]
    }
   ],
   "source": [
    "# Subset and Split Data\n",
    "X = df.loc[:,['ADMIT_ID']+[c for c in df.columns if (c.startswith('AGE') or c.startswith('DIAGNOSIS_') or c.startswith('PROCEDURE_'))]]\n",
    "y = df.loc[:, ['los_binary']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# Train Model\n",
    "baseline_model = XGBClassifier()\n",
    "baseline_model.fit(X_train, y_train.iloc[:,0])\n",
    "baseline_y_pred = baseline_model.predict(X_test)\n",
    "baseline_y_prob = baseline_model.predict_proba(X_test)[:, 1]\n",
    "#\n",
    "print('\\n', \"Baseline ROC_AUC Score:\", sk_metrics.roc_auc_score(y_test.iloc[:,0], baseline_y_prob) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Testing Gender as a Sensitive Attribute <a class=\"anchor\" id=\"part2\"></a>\n",
    "Our first experiment will test the effect of including the sensitive attribute 'GENDER_M'. This attribute is encoded in our data as a boolean attribute, where 0=female and 1=male, since males are assumed to be the privileged group. For the purposes of this experiment all other senstitive attributes and potential proxies will be dropped, such that only gender, age, diangosis, and procedure codes will be used to make the prediction.\n",
    "\n",
    "First we will examine fairness measurements for a version of this model that includes gender as a feature, before comparing them to similar measurements for the baseline (without gender). We will see that while some measures can be used to analyze a model in isolation, others require comparison against other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GENDER_M</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9987.0</td>\n",
       "      <td>9.1373</td>\n",
       "      <td>6.1984</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>4.7507</td>\n",
       "      <td>7.6722</td>\n",
       "      <td>12.0667</td>\n",
       "      <td>29.9889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12447.0</td>\n",
       "      <td>9.0975</td>\n",
       "      <td>6.2171</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>4.7149</td>\n",
       "      <td>7.4597</td>\n",
       "      <td>11.9729</td>\n",
       "      <td>29.9660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count    mean     std     min     25%     50%      75%      max\n",
       "GENDER_M                                                                   \n",
       "0          9987.0  9.1373  6.1984  0.0042  4.7507  7.6722  12.0667  29.9889\n",
       "1         12447.0  9.0975  6.2171  0.0049  4.7149  7.4597  11.9729  29.9660"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('GENDER_M')['length_of_stay'].describe().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ROC_AUC Score with Gender Included: 0.7236032721546329\n"
     ]
    }
   ],
   "source": [
    "# Update Split Data to Include Gender as a Feature\n",
    "X_train_gender = X_train.join(df[['GENDER_M']], how='inner')\n",
    "X_test_gender = X_test.join(df[['GENDER_M']], how='inner')\n",
    "# Train New Model with Gender Feature\n",
    "gender_model = XGBClassifier()\n",
    "gender_model.fit(X_train_gender, y_train.iloc[:,0])\n",
    "y_pred_gender = gender_model.predict(X_test_gender)\n",
    "#\n",
    "print('\\n', \"ROC_AUC Score with Gender Included:\", sk_metrics.roc_auc_score(y_test.iloc[:,0], y_pred_gender) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Fairness via AIF360\n",
    "\n",
    "AIF360 requires the sensitive attribute to be in the same dataframe (or 2-D array) as the target variable (both the ground truth and the prediction), so we add that here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_aif = pd.concat([X_test_gender['GENDER_M'], y_test], axis=1).set_index('GENDER_M')\n",
    "y_pred_aif = pd.concat([X_test_gender['GENDER_M'].reset_index(drop=True), pd.Series(y_pred_gender)], axis=1).set_index('GENDER_M')\n",
    "y_pred_aif.columns = y_test_aif.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Rates\n",
    "The base rate is the average value of the ground truth (optionally weighted). It provides useful context, although it is not technically a measure of fairness. \n",
    "> $base\\_rate = \\sum_{i=0}^N(y_i)/N$\n",
    "\n",
    "The Selection Rate is the average predicted value. For a binary prediction problem it equates to the probability of prediction for the positive class (\"the probability of selection\").\n",
    "> $selection\\_rate = \\sum_{i=0}^N(ŷ_i)/N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_rate: 0.38006482982171796\n",
      "selection_rate: 0.2693138843868179\n"
     ]
    }
   ],
   "source": [
    "print(\"base_rate:\", \n",
    "      base_rate(y_test_aif, y_pred_aif)\n",
    "     )\n",
    "print('selection_rate:', \n",
    "      selection_rate(y_test_aif, y_pred_aif)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Demographic Parity\n",
    "\n",
    "The Disparate Impact Ratio is the ratio between the probability of positive prediction for the unprivileged group and the probability of positive prediction for the privileged group. A ratio of 1 indicates that the model is fair relative to the sensitive attribute (it favors neither group).\n",
    "> $disparate\\_impact\\_ratio = \\dfrac{P(ŷ =1 | unprivileged)}{P(ŷ =1 | privileged)} = \\dfrac{selection\\_rate(ŷ_{unprivileged})}{selection\\_rate(ŷ_{privileged})}$\n",
    "\n",
    "Statistical Parity Difference is the difference in the probability of prediction between the two groups. A difference of 0 indicates that the model is fair relative to the sensitive attribute (it favors neither group).\n",
    "> $statistical\\_parity\\_difference = P(ŷ =1 | unprivileged) - P(ŷ =1 | privileged) = selection\\_rate(ŷ_{unprivileged}) - selection\\_rate(ŷ_{privileged}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disparate_impact_ratio \t 1.0991277478515271\n",
      "statistical_parity_difference \t 0.025555420864156297\n"
     ]
    }
   ],
   "source": [
    "print('disparate_impact_ratio', '\\t', \n",
    "      disparate_impact_ratio(y_test_aif, y_pred_aif, prot_attr='GENDER_M')\n",
    "     )\n",
    "print('statistical_parity_difference', '\\t', \n",
    "      statistical_parity_difference(y_test_aif, y_pred_aif, prot_attr='GENDER_M')\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Equal Odds\n",
    "Average Odds Difference measures the average of the difference in False Positive Rate (FPR) and True Positive Rate (TPR) for the unprivileged and privileged groups. A difference of 1 indicates that the model is fair relative to the sensitive attribute.\n",
    "> $ average\\_odds\\_difference = \\dfrac{(FPR_{unprivileged} - FPR_{privileged})\n",
    "        + (TPR_{unprivileged} - TPR_{privileged})}{2}$\n",
    "\n",
    "Average Odds Error is the average of the absolute difference in FPR and TPR for the unprivileged and privileged groups. A difference of 1 indicates that the model is fair relative to the sensitive attribute.\n",
    "> $average\\_odds\\_error = \\dfrac{|FPR_{unprivileged} - FPR_{privileged}|\n",
    "        + |TPR_{unprivileged} - TPR_{privileged}|}{2}$\n",
    "        \n",
    "Equal Opportunity Difference is the difference in recall scores (TPR) between the unprivileged and privileged groups. A difference of 0 indicates that the model is fair relative to the sensitive attribute.\n",
    "> $equal\\_opportunity\\_difference =  Recall(ŷ_{unprivileged}) - Recall(ŷ_{privileged})$\n",
    "\n",
    "\n",
    "> to do: add range for interpretation (eg. \"if positive, indicates that the privileged group is favored\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_odds_difference \t 0.01336732030838983\n",
      "average_odds_error \t 0.015130813452901792\n",
      "equal_opportunity_difference \t 0.028498133761291622\n"
     ]
    }
   ],
   "source": [
    "print('average_odds_difference', '\\t', \n",
    "       average_odds_difference(y_test_aif, y_pred_aif, prot_attr='GENDER_M'))\n",
    "print('average_odds_error', '\\t', \n",
    "       average_odds_error(y_test_aif, y_pred_aif, prot_attr='GENDER_M')\n",
    "     )\n",
    "print('equal_opportunity_difference', '\\t', \n",
    "       equal_opportunity_difference(y_test_aif, y_pred_aif, prot_attr='GENDER_M')\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Measures of Disparate Performance\n",
    "<a id='aif_difference_func'></a>\n",
    "Both of the libraries we will explore have added features to calculate the between-group difference in performance. In AIF360, this is facilitated by a flexible \"difference\" method that accepts an arbitrary scoring function as an argument. Here we use the function to demonstrate the Between-Group AUC Difference and Between-Group Balanced Accuracy Difference.\n",
    "\n",
    "AIF360 also contains a \"ratio\" function of similar purpose. Here it's demonstrated with two arbitrary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between-Group AUC Difference \t 0.009925880264296971\n",
      "Between-Group Balanced Accuracy Difference \t 0.015130813452901792\n",
      "Selection Rate Ratio (Disparate Impact Ratio) 1.0991277478515271\n",
      "Recall Ratio 1.0534610559927016\n"
     ]
    }
   ],
   "source": [
    "# Examples Using the Difference Function\n",
    "y_prob_gender = gender_model.predict_proba(X_test_gender)[:, 1]\n",
    "print('Between-Group AUC Difference', '\\t',\n",
    "          difference(sk_metrics.roc_auc_score, y_test_aif, y_prob_gender, prot_attr='GENDER_M', priv_group=1)\n",
    "     )\n",
    "print('Between-Group Balanced Accuracy Difference', '\\t',\n",
    "          difference(sk_metrics.balanced_accuracy_score, y_test_aif, y_pred_gender, prot_attr='GENDER_M', priv_group=1)\n",
    "     )\n",
    "# Examples Using the Ratio Function\n",
    "print('Selection Rate Ratio (Disparate Impact Ratio)', \n",
    "        ratio(selection_rate, y_test_aif, y_pred_gender, prot_attr='GENDER_M', priv_group=1) \n",
    "     )\n",
    "print('Recall Ratio', \n",
    "          ratio(sk_metrics.recall_score, y_test_aif, y_pred_gender, prot_attr='GENDER_M', priv_group=1) \n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures Of Individual Fairness\n",
    "Consistency scores measure the similarity between a given prediction and the predictions of \"like\" individuals. In AIF360, the consistency score is calculated as the compliment of the mean distance to the score of the mean nearest neighbhor, using Scikit's Nearest Neighbors algorithm (default 5 neighbors determined by BallTree algorithm).\n",
    "> $ consistency\\_score = 1 - |mean_{distance}(mean({nearest\\ neighbor}) )| $\n",
    "\n",
    "#### The Generalized Entropy Index and Related Measures\n",
    "The Generalized Entropy (GE) Index is...\n",
    "> $ GE =  \\mathcal{E}(\\alpha) = \\begin{cases}\n",
    "            \\frac{1}{n \\alpha (\\alpha-1)}\\sum_{i=1}^n\\left[\\left(\\frac{b_i}{\\mu}\\right)^\\alpha - 1\\right],& \\alpha \\ne 0, 1,\\\\\n",
    "            \\frac{1}{n}\\sum_{i=1}^n\\frac{b_{i}}{\\mu}\\ln\\frac{b_{i}}{\\mu},& \\alpha=1,\\\\\n",
    "            -\\frac{1}{n}\\sum_{i=1}^n\\ln\\frac{b_{i}}{\\mu},& \\alpha=0.\n",
    "        \\end{cases}\n",
    "        $\n",
    "\n",
    "Generalized Entropy Error = Calculates the GE of the set of errors, i.e. 1 + (ŷ == pos_label) - (y == pos_label) \n",
    "> $ GE(Error) = b_i = \\hat{y}_i - y_i + 1 $\n",
    "\n",
    "Between Group Generalized Entropy Error = Calculates the GE of the set of mean errors for the two groups (privileged error & unprivileged error), weighted by the number of predictions in each group\n",
    "> $ GE(Error_{group}) =  GE( [N_{unprivileged}*mean(Error_{unprivileged}), N_{privileged}*mean(Error_{privileged})] ) $\n",
    "\n",
    "> to do: refine this text and add interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consistency_score \t 0.6829551593733116\n",
      "generalized_entropy_error \t 0.1401567775778093\n",
      "between_group_generalized_entropy_error \t 1.4508160900911056e-05\n"
     ]
    }
   ],
   "source": [
    "print('consistency_score', '\\t',\n",
    "      consistency_score(X_test_gender, y_pred_gender)\n",
    "     )\n",
    "print('generalized_entropy_error', '\\t',\n",
    "      generalized_entropy_error(y_test['los_binary'], y_pred_gender)\n",
    "     )\n",
    "print('between_group_generalized_entropy_error', '\\t',\n",
    "      between_group_generalized_entropy_error(y_test_aif, y_pred_aif, prot_attr=['GENDER_M'])\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Against a Second Model - Evaluating Unawareness\n",
    "<a class=\"anchor\" id=\"part3\"></a>\n",
    "\n",
    "To demonstrate the change in model scores relative to the use of a sensitive attribute, we will compare the above scores to those of our baseline model. Although the GENDER_M feature is not included in our baseline, since we attached it to the y_test_aif dataframe above we can still evaluate it's bias relative to GENDER_M. As shown below, there is no significant difference in the scores of these two models. Therefore, the inclusion of GENDER_M as a feature does not contribute to gender bias for these models.\n",
    "\n",
    "This tutorial includes a helper function which returns all of the previously-seen measures in a convenient pandas dataframe. Since we have already discussed the measures individuallly, we will use this function  to save space for the rest of the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure</th>\n",
       "      <th>gender_score</th>\n",
       "      <th>gender_score (feature removed)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selection_rate</td>\n",
       "      <td>0.2693</td>\n",
       "      <td>0.2693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disparate_impact_ratio</td>\n",
       "      <td>1.0991</td>\n",
       "      <td>1.0991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>statistical_parity_difference</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>average_odds_difference</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>average_odds_error</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>equal_opportunity_difference</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>generalized_entropy_error</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.1402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>between_group_generalized_entropy_error</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>consistency_score</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.6830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Between-Group AUC Difference</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Between-Group Balanced Accuracy Difference</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       measure  gender_score  \\\n",
       "0                               selection_rate        0.2693   \n",
       "1                       disparate_impact_ratio        1.0991   \n",
       "2                statistical_parity_difference        0.0256   \n",
       "3                      average_odds_difference        0.0134   \n",
       "4                           average_odds_error        0.0151   \n",
       "5                 equal_opportunity_difference        0.0285   \n",
       "6                    generalized_entropy_error        0.1402   \n",
       "7      between_group_generalized_entropy_error        0.0000   \n",
       "8                            consistency_score        0.6830   \n",
       "9                 Between-Group AUC Difference        0.0099   \n",
       "10  Between-Group Balanced Accuracy Difference        0.0151   \n",
       "\n",
       "    gender_score (feature removed)  \n",
       "0                           0.2693  \n",
       "1                           1.0991  \n",
       "2                           0.0256  \n",
       "3                           0.0134  \n",
       "4                           0.0151  \n",
       "5                           0.0285  \n",
       "6                           0.1402  \n",
       "7                           0.0000  \n",
       "8                           0.6830  \n",
       "9                           0.0099  \n",
       "10                          0.0151  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Tutorial Helper Function to Generate Dataframe of Measure Values for Gender-Included Model, Relative to Patient Gender\n",
    "gender_scores = tutorial_helpers.get_aif360_measures_df(X_test_gender, y_test, y_pred_gender, y_prob_gender, sensitive_attributes=['GENDER_M'])\n",
    "\n",
    "# Use Tutorial Helper Function to Generate Dataframe of Measure Values for Baseline Model, Relative to Patient Gender\n",
    "baseline_scores = tutorial_helpers.get_aif360_measures_df(X_test_gender, y_test, baseline_y_pred, baseline_y_prob, sensitive_attributes=['GENDER_M'])\n",
    "\n",
    "# Merge Results to Compare Values\n",
    "comparison = gender_scores.rename(columns={'value':'gender_score'}\n",
    "                                ).merge(baseline_scores.rename(columns={'value':'gender_score (feature removed)'}))\n",
    "comparison.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Testing Other Sensitive Attributes\n",
    "\n",
    "Our next experiment will test the presence of bias relative to a patient\\'s language, assuming that there is a bias toward individuals who speak English. As above, we will add a boolean 'LANGUAGE_ENGL' feature to the baseline data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANG_ENGL</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10266.0</td>\n",
       "      <td>9.5874</td>\n",
       "      <td>6.4989</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>4.9115</td>\n",
       "      <td>7.9167</td>\n",
       "      <td>12.8132</td>\n",
       "      <td>29.9792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12168.0</td>\n",
       "      <td>8.7169</td>\n",
       "      <td>5.9239</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>4.4819</td>\n",
       "      <td>7.2174</td>\n",
       "      <td>11.6069</td>\n",
       "      <td>29.9889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count    mean     std     min     25%     50%      75%      max\n",
       "LANG_ENGL                                                                   \n",
       "0          10266.0  9.5874  6.4989  0.0042  4.9115  7.9167  12.8132  29.9792\n",
       "1          12168.0  8.7169  5.9239  0.0049  4.4819  7.2174  11.6069  29.9889"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update Split Data to Include Language as a Feature\n",
    "lang_cols = [c for c in df.columns if c.startswith(\"LANGUAGE_\")]\n",
    "eng_cols = ['LANGUAGE_ENGL']\n",
    "X_lang =  df.loc[:,lang_cols]\n",
    "X_lang['LANG_ENGL'] = 0\n",
    "X_lang.loc[X_lang[eng_cols].eq(1).any(axis=1), 'LANG_ENGL'] = 1\n",
    "X_lang = X_lang.drop(lang_cols, axis=1).fillna(0)\n",
    "\n",
    "# Show LOS Statistics Relative to LANG_ENGL\n",
    "X_lang.join(df['length_of_stay']).groupby('LANG_ENGL')['length_of_stay'].describe().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train New Model with Language Feature\n",
    "X_lang_train = X_train.join(X_lang, how='inner')\n",
    "X_lang_test = X_test.join(X_lang, how='inner')\n",
    "lang_model = XGBClassifier()\n",
    "lang_model.fit(X_lang_train, y_train.iloc[:,0])\n",
    "y_pred_lang = lang_model.predict(X_lang_test)\n",
    "y_prob_lang = lang_model.predict_proba(X_lang_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, by comparing the results with and without the sensitivie attribute we can better demonstrate the effect that the attribute has on the fairness of the model. In this example we see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure Values with LANG_ENGL Included:\n",
      "test\n",
      "test2\n",
      "                         measure   value\n",
      "0                 selection_rate  0.2704\n",
      "1         disparate_impact_ratio  1.2227\n",
      "2  statistical_parity_difference  0.0546\n",
      "\n",
      " Measure Values with LANG_ENGL Removed:\n",
      "test\n",
      "test2\n",
      "                         measure   value\n",
      "0                 selection_rate  0.2693\n",
      "1         disparate_impact_ratio  1.0930\n",
      "2  statistical_parity_difference  0.0240\n"
     ]
    }
   ],
   "source": [
    "# Generate Dataframe of  Measure Values for Language-Inclusive Model, Relative to Patient Language\n",
    "print(\"Measure Values with LANG_ENGL Included:\")\n",
    "lang_scores = tutorial_helpers.get_aif360_measures_df(X_lang_test, y_test, y_pred_lang, y_prob_lang, sensitive_attributes=['LANG_ENGL'])\n",
    "print(lang_scores.round(4).head(3))\n",
    "\n",
    "# Generate Dataframe of Measure Values for Baseline Model, Relative to Patient Language\n",
    "print(\"\\n\", \"Measure Values with LANG_ENGL Removed:\")\n",
    "lang_ko_scores = tutorial_helpers.get_aif360_measures_df(X_lang_test, y_test, baseline_y_pred, baseline_y_prob, sensitive_attributes=['LANG_ENGL']) \n",
    "print(lang_ko_scores.round(4).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing All Four Models Against Each Other\n",
    "As shown below, exclusion of the LANG_ENGL feature has a more significant impact on the fairness of the model than does exclusion of GENDER_M (relative to their specific biases). Moreover, using the 80/20 rule we can see that inclusion of LANG_ENGL leads to what can be considered a \"significant\" bias, as shown by the Disparate Impact Ratio. In this case, predictions for those individuals who do not speak English are significantly more likely to be above the mean, even though this difference is not [currently] reflected in the ground truth.\n",
    "\n",
    "> to do: validate this conclusion after fixing the issue with LOS <- currently the average LOS in the dataset is still higher than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure</th>\n",
       "      <th>gender_score</th>\n",
       "      <th>gender_score (feature removed)</th>\n",
       "      <th>lang_score</th>\n",
       "      <th>lang_score (feature removed)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selection_rate</td>\n",
       "      <td>0.2693</td>\n",
       "      <td>0.2693</td>\n",
       "      <td>0.2704</td>\n",
       "      <td>0.2693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disparate_impact_ratio</td>\n",
       "      <td>1.0991</td>\n",
       "      <td>1.0991</td>\n",
       "      <td>1.2227</td>\n",
       "      <td>1.0930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>statistical_parity_difference</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.0240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>average_odds_difference</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>average_odds_error</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>equal_opportunity_difference</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>0.0061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>generalized_entropy_error</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.1402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>between_group_generalized_entropy_error</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>consistency_score</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.6815</td>\n",
       "      <td>0.6828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Between-Group AUC Difference</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>-0.0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Between-Group Balanced Accuracy Difference</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       measure  gender_score  \\\n",
       "0                               selection_rate        0.2693   \n",
       "1                       disparate_impact_ratio        1.0991   \n",
       "2                statistical_parity_difference        0.0256   \n",
       "3                      average_odds_difference        0.0134   \n",
       "4                           average_odds_error        0.0151   \n",
       "5                 equal_opportunity_difference        0.0285   \n",
       "6                    generalized_entropy_error        0.1402   \n",
       "7      between_group_generalized_entropy_error        0.0000   \n",
       "8                            consistency_score        0.6830   \n",
       "9                 Between-Group AUC Difference        0.0099   \n",
       "10  Between-Group Balanced Accuracy Difference        0.0151   \n",
       "\n",
       "    gender_score (feature removed)  lang_score  lang_score (feature removed)  \n",
       "0                           0.2693      0.2704                        0.2693  \n",
       "1                           1.0991      1.2227                        1.0930  \n",
       "2                           0.0256      0.0546                        0.0240  \n",
       "3                           0.0134      0.0382                        0.0047  \n",
       "4                           0.0151      0.0382                        0.0047  \n",
       "5                           0.0285      0.0516                        0.0061  \n",
       "6                           0.1402      0.1406                        0.1402  \n",
       "7                           0.0000      0.0000                        0.0001  \n",
       "8                           0.6830      0.6815                        0.6828  \n",
       "9                           0.0099     -0.0034                       -0.0015  \n",
       "10                          0.0151      0.0133                        0.0014  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_comparison = comparison.merge(lang_scores.rename(columns={'value':'lang_score'})\n",
    "                            ).merge(lang_ko_scores.rename(columns={'value':'lang_score (feature removed)'})\n",
    "                            )\n",
    "full_comparison.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparison to FairLearn <a class=\"anchor\" id=\"part5\"></a>\n",
    "\n",
    "The FairLearn and AIF360 APIs for Scikit and XGBOOST models are very similar in user experience, and contain a similar set of measures as shown in the table below. Although the set of measures provided by AIF360 is more comprehensive, FairLearn does provide some measures that are unique. First we'll look at FairLearn measures that are also found in AIF360 before explaining the measures that are distinct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/[path to image].png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"img/[path to image].png\", width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection rate 0.27039438141545113\n",
      "Demographic parity difference 0.054553219850345586\n",
      "Demographic parity ratio 0.8178916255964723\n"
     ]
    }
   ],
   "source": [
    "## Display Example Results for Measures that are Found in AIF360\n",
    "print(\"Selection rate\", \n",
    "      selection_rate(y_test, y_pred_lang) )\n",
    "print(\"Demographic parity difference\", \n",
    "      demographic_parity_difference(y_test, y_pred_lang, sensitive_features=X_lang_test['LANG_ENGL']))\n",
    "print(\"Demographic parity ratio\", \n",
    "      demographic_parity_ratio(y_test, y_pred_lang, sensitive_features=X_lang_test['LANG_ENGL'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures Specific to FairLearn\n",
    "Two measures that are not present in AIF360 are the Equalized Odds Difference and the Equalized Odds Ratio. Similar to the Average Odds Difference and Average Odds Ratio found in AIF360, these are measures of disparate rates in prediction.\n",
    "\n",
    "The Equalized Odds Difference is the greater between the difference in TPR and the difference in FPR. A value of 0 indicates that all groups have the same TPR, FPR, TNR, and FNR, and that the model is \"fair\" relative to the sensitive attribute.\n",
    "> $ equalized\\_odds\\_difference = max( (FPR_{unprivileged} - FPR_{privileged}), (TPR_{unprivileged} - TPR_{privileged}) )$\n",
    "\n",
    "\n",
    "The Equalized Odds Ratio is the smaller between the TPR Ratio and FPR Ratio, where the ratios are defined as the ratio of the smaller of the between-group rates vs the larger of the between-group rates.  A value of 1 means that all groups have the same TPR, FPR, TNR, and FNR.\n",
    "> $ equalized\\_odds\\_ratio = min( \\dfrac{FPR_{smaller}}{FPR_{larger}}, \\dfrac{TPR_{smaller}}{TPR_{larger}} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized Odds Difference 0.05159445477325997\n",
      "Equalized Odds Ratio 0.7831711964697623\n"
     ]
    }
   ],
   "source": [
    "print(\"Equalized Odds Difference\",\n",
    "        equalized_odds_difference(y_test, y_pred_lang, sensitive_features=X_lang_test['LANG_ENGL']))\n",
    "print(\"Equalized Odds Ratio\",\n",
    "        equalized_odds_ratio(y_test, y_pred_lang, sensitive_features=X_lang_test['LANG_ENGL']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Summaries\n",
    "Similar to AIF360's \"difference\", which we used [above](#aif_difference_func) to calculate differences in prediction scores, FairLearn provides a \"group_summary\" function that returns a dictionary with both the overall and the between-group scores for a predictive metric. Also available are convenient wrappers, such as the \"balanced_accuracy_score_group_summary\" shown below which returns a summary of the balanced accuracy scores.\n",
    "\n",
    "To extend the summary functionality, FairLearn also offers a \"difference_from_summary\" function (shown below), which calculates the between-group prediction difference (again, as we calculated [above](#aif_difference_func))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy Summary {'overall': 0.7227318124596438, 'by_group': {0: 0.7289100106775893, 1: 0.7155632936639851}}\n",
      "Between-Group Balanced Accuracy Difference 0.013346717013604237\n"
     ]
    }
   ],
   "source": [
    "balanced_accuracy_summary = balanced_accuracy_score_group_summary(y_test, y_pred_lang, sensitive_features=X_lang_test['LANG_ENGL'])\n",
    "print(\"Balanced Accuracy Summary\", balanced_accuracy_summary )\n",
    "print(\"Between-Group Balanced Accuracy Difference\", difference_from_summary(balanced_accuracy_summary) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This tutorial introduced multiple measures of ML fairness in the context of a healthcare model using the AIF360 and FairLearn Python libraries. A subset of the MIMIC-III database was used to generate a series of simple Length of Stay (LOS) models. It was shown that while the inclusion of a sensitive feature can significantly affect a model's bias as it relates to that feature, this is not always the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "* AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. Bellamy RK, Dey K, Hind M, Hoffman SC, Houde S, Kannan K, ... & Nagar S (2018). arXiv Preprint. [arXiv:1810.01943.](https://arxiv.org/abs/1810.01943)\n",
    "> See Also [AIF360 Documentation](http://aif360.mybluemix.net/) \n",
    "\n",
    "* HCUP CCS. Healthcare Cost and Utilization Project (HCUP) (2017). Agency for Healthcare Research and Quality, Rockville, MD. Available at [www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp)\n",
    "\n",
    "* Fairlearn: A toolkit for assessing and improving fairness in AI. Bird S, Dudík M,  Wallach H,  Walker K (2020). Microsoft Research. Available at [https://www.microsoft.com/en-us/research/uploads/prod/2020/05/Fairlearn_whitepaper.pdf](https://www.microsoft.com/en-us/research/uploads/prod/2020/05/Fairlearn_whitepaper.pdf)\n",
    "> See Also [FairLearn Reference](https://fairlearn.github.io/).\n",
    "\n",
    "* MIMIC-III, a freely accessible critical care database. Johnson AEW, Pollard TJ, Shen L, Lehman L, Feng M, Ghassemi M, Moody B, Szolovits P, Celi LA, and Mark RG. Scientific Data (2016). DOI: 10.1038/sdata.2016.35. Available at [http://www.nature.com/articles/sdata201635](http://www.nature.com/articles/sdata201635)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
