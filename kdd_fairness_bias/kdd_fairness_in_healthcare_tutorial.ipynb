{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial of Fairness Metrics for Healthcare\n",
    "\n",
    "This tutorial introduces methods and libraries for measuring fairness and bias in machine learning models as as they relate to problems in healthcare. After providing some background, it will generate a simple baseline model predicting Length of Stay (LOS) using data from the [MIMIC-III database](https://mimic.physionet.org/gettingstarted/access/). It will then use variations of that model to demonstrate common measures of \"fairness\" using [AIF360](http://aif360.mybluemix.net/), a prominent library for this purpose, before comparing AIF360 to another prominent library, [FairLearn](https://fairlearn.github.io/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Contents\n",
    "[Part 0:] Background\n",
    "\n",
    "[Part 1:](#part1) Model Setup\n",
    "\n",
    "[Part 2:](#part2) Metrics of Fairness in AIF360\n",
    "\n",
    "[Part 3:](#part3) Comparing Against a Second Model - Evaluating Unawarenes\n",
    "\n",
    "[Part 4:](#part4) Testing Other Sensitive Attributes\n",
    "\n",
    "[Part 5:](#part5) Comparison to FairLearn\n",
    "\n",
    "### Requirements\n",
    "This tutorial assumes basic knowledge of machine learning implementation in Python. Before starting, please install [AIF360](http://aif360.mybluemix.net/) and [FairLearn](https://fairlearn.github.io/). Also, ensure that you have installed the Pandas, Numpy, Scikit, and XGBOOST libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Background \n",
    "SECTIONS TO BE INCLUDED:\n",
    "* what is fairness\n",
    "* metrics for fairness\n",
    "* list of measures that will be included in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Model Setup <a class=\"anchor\" id=\"part1\"></a>\n",
    "\n",
    "This section introduces and loads the data subset that will be used in this tutorial. Then it generates a simple baseline model to be used throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Jupyter Add-Ons from local folder\n",
    "import tutorial_helpers\n",
    "\n",
    "# Prediction Libs\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# Metrics\n",
    "from aif360.sklearn.metrics import *\n",
    "from fairlearn.metrics import (\n",
    "    selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "    balanced_accuracy_score_group_summary, roc_auc_score_group_summary,\n",
    "    equalized_odds_difference, difference_from_summary)\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC III\n",
    "This tutorial uses data from the MIMIC III Critical Care database, a freely accessible source of Electronic Health Records from Beth Israel Deaconess Medical Center in Boston, years 2001 through 2012. To download the MIMIC III data, please use this link: [Access to MIMIC III](https://mimic.physionet.org/gettingstarted/access/). Please save the data in a folder with the default name (\"MIMIC\").\n",
    "\n",
    "The raw MIMIC download contains only a folder of zipped_files. The tutorial code will automatically unzip and format the necessary data for this experiment, saving the formatted data in the current folder. Simply enter the correct path of the MIMIC folder in the following cell to enable this feature. Your path should end with the directory \"MIMIC\".\n",
    "\n",
    "Example: path_to_mimic_data_folder = \"~/data/MIMIC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_mimic_data_folder = \"[path to your downloaded data folder]\"\n",
    "path_to_mimic_data_folder = \"~/data/MIMIC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Subset\n",
    "The following models use data from all years of the MIMIC-III dataset for patients aged 65 and older. Features include diagnosis and procedure codes categorized through the Clinical Classifications Software system ([HCUP](#hcup)). \n",
    "\n",
    "Data are imported at the encounter level, with patient identification dropped. All features are one-hot encoded and prefixed with their variable type (e.g. \"GENDER_\", \"ETHNICITY_\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   ADMIT_ID   AGE  length_of_stay  GENDER_M  \\\n0   1019779  65.0        1.144444         0   \n1   1006687  70.0        5.496528         1   \n2    978785  75.0        6.768056         1   \n5   1052125  70.0        6.988889         1   \n7   1017033  75.0        5.364583         1   \n\n   ETHNICITY_AMERICAN INDIAN/ALASKA NATIVE  \\\n0                                        0   \n1                                        0   \n2                                        0   \n5                                        0   \n7                                        0   \n\n   ETHNICITY_AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE  \\\n0                                                  0                    \n1                                                  0                    \n2                                                  0                    \n5                                                  0                    \n7                                                  0                    \n\n   ETHNICITY_ASIAN  ETHNICITY_ASIAN - ASIAN INDIAN  \\\n0                0                               0   \n1                0                               0   \n2                0                               0   \n5                0                               0   \n7                0                               0   \n\n   ETHNICITY_ASIAN - CAMBODIAN  ETHNICITY_ASIAN - CHINESE  ...  \\\n0                            0                          0  ...   \n1                            0                          0  ...   \n2                            0                          0  ...   \n5                            0                          0  ...   \n7                            0                          0  ...   \n\n   PROCEDURE_CCS_221  PROCEDURE_CCS_222  PROCEDURE_CCS_223  PROCEDURE_CCS_224  \\\n0                  0                  0                  0                  0   \n1                  0                  1                  0                  0   \n2                  0                  0                  0                  0   \n5                  0                  0                  0                  0   \n7                  0                  1                  0                  0   \n\n   PROCEDURE_CCS_225  PROCEDURE_CCS_226  PROCEDURE_CCS_227  PROCEDURE_CCS_228  \\\n0                  0                  0                  0                  0   \n1                  0                  0                  0                  0   \n2                  0                  0                  0                  0   \n5                  0                  0                  1                  0   \n7                  0                  0                  1                  0   \n\n   PROCEDURE_CCS_229  PROCEDURE_CCS_231  \n0                  0                  0  \n1                  0                  0  \n2                  0                  0  \n5                  0                  0  \n7                  0                  0  \n\n[5 rows x 650 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ADMIT_ID</th>\n      <th>AGE</th>\n      <th>length_of_stay</th>\n      <th>GENDER_M</th>\n      <th>ETHNICITY_AMERICAN INDIAN/ALASKA NATIVE</th>\n      <th>ETHNICITY_AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE</th>\n      <th>ETHNICITY_ASIAN</th>\n      <th>ETHNICITY_ASIAN - ASIAN INDIAN</th>\n      <th>ETHNICITY_ASIAN - CAMBODIAN</th>\n      <th>ETHNICITY_ASIAN - CHINESE</th>\n      <th>...</th>\n      <th>PROCEDURE_CCS_221</th>\n      <th>PROCEDURE_CCS_222</th>\n      <th>PROCEDURE_CCS_223</th>\n      <th>PROCEDURE_CCS_224</th>\n      <th>PROCEDURE_CCS_225</th>\n      <th>PROCEDURE_CCS_226</th>\n      <th>PROCEDURE_CCS_227</th>\n      <th>PROCEDURE_CCS_228</th>\n      <th>PROCEDURE_CCS_229</th>\n      <th>PROCEDURE_CCS_231</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1019779</td>\n      <td>65.0</td>\n      <td>1.144444</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1006687</td>\n      <td>70.0</td>\n      <td>5.496528</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>978785</td>\n      <td>75.0</td>\n      <td>6.768056</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1052125</td>\n      <td>70.0</td>\n      <td>6.988889</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1017033</td>\n      <td>75.0</td>\n      <td>5.364583</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 650 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df = tutorial_helpers.load_example_data(path_to_mimic_data_folder) # note: ADMIT_ID has been masked\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Length of Stay Model\n",
    "All models in this tutorial predict the length of time spent in the ICU, a.k.a. the \"Length of Stay\" (LOS). The baseline model will use only the patient's age, their diagnosis, and the use of medical procedures during their stay to predict this value.\n",
    "\n",
    "Two target variables will be used for the following experiments: 'length_of_stay' and 'los_binary'. For this dataset, length_of_stay is, of course, the true value of the length of the patient's stay in days. The los_binary variable is a binary variable indicating whether the admission resulted in a length of stay either < or >= the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       length_of_stay  los_binary\ncount      22434.0000  22434.0000\nmean           9.1152      0.3880\nstd            6.2087      0.4873\nmin            0.0042      0.0000\n25%            4.7352      0.0000\n50%            7.5799      0.0000\n75%           12.0177      1.0000\nmax           29.9889      1.0000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>length_of_stay</th>\n      <th>los_binary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>22434.0000</td>\n      <td>22434.0000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>9.1152</td>\n      <td>0.3880</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>6.2087</td>\n      <td>0.4873</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.0042</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>4.7352</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>7.5799</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>12.0177</td>\n      <td>1.0000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>29.9889</td>\n      <td>1.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "mean_val=df['length_of_stay'].mean()\n",
    "df['los_binary'] = df['length_of_stay'].apply(lambda x: 0 if x<=mean_val else 1)\n",
    "df[['length_of_stay', 'los_binary']].describe().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n Baseline ROC_AUC Score: 0.7236032721546329\n"
    }
   ],
   "source": [
    "# Subset and split data for the first model\n",
    "X = df.loc[:,['ADMIT_ID']+[c for c in df.columns if (c.startswith('AGE') or c.startswith('DIAGNOSIS_') or c.startswith('PROCEDURE_'))]]\n",
    "y = df.loc[:, ['los_binary']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# generate alternative model\n",
    "baseline_model = XGBClassifier()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "baseline_y_pred = baseline_model.predict(X_test)\n",
    "\n",
    "#\n",
    "print('\\n', \"Baseline ROC_AUC Score:\", roc_auc_score(y_test, baseline_y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Testing Gender as a Sensitive Attribute <a class=\"anchor\" id=\"part2\"></a>\n",
    "Our first experiment will test the effect of including the sensitive attribute 'GENDER_M'. This attribute is encoded in our data as a boolean attribute, where 0=female and 1=male, since males are assumed to be the privileged group. For the purposes of this experiment all other senstitive attributes and potential proxies will be dropped, such that only gender, diangosis, and procedure codes will be used to make the prediction.\n",
    "\n",
    "First we will examine fairness measurements for a version of this model that includes gender as a feature, before comparing them to similar measurements for the baseline (without gender). We will see that while some measures can be used to analyze a model in isolation, others require comparison against other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            count    mean     std     min     25%     50%      75%      max\nGENDER_M                                                                   \n0          9987.0  9.1373  6.1984  0.0042  4.7507  7.6722  12.0667  29.9889\n1         12447.0  9.0975  6.2171  0.0049  4.7149  7.4597  11.9729  29.9660",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th>GENDER_M</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9987.0</td>\n      <td>9.1373</td>\n      <td>6.1984</td>\n      <td>0.0042</td>\n      <td>4.7507</td>\n      <td>7.6722</td>\n      <td>12.0667</td>\n      <td>29.9889</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12447.0</td>\n      <td>9.0975</td>\n      <td>6.2171</td>\n      <td>0.0049</td>\n      <td>4.7149</td>\n      <td>7.4597</td>\n      <td>11.9729</td>\n      <td>29.9660</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df.groupby('GENDER_M')['length_of_stay'].describe().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n ROC_AUC Score with Gender Included: 0.7236032721546329\n"
    }
   ],
   "source": [
    "# Generate a model that includes gender as a feature\n",
    "X_train_gender = X_train.join(df[['GENDER_M']], how='inner')\n",
    "X_test_gender = X_test.join(df[['GENDER_M']], how='inner')\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train_gender, y_train)\n",
    "y_pred_gender = model.predict(X_test_gender)\n",
    "\n",
    "#\n",
    "print('\\n', \"ROC_AUC Score with Gender Included:\", roc_auc_score(y_test, y_pred_gender) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Fairness via AIF360\n",
    "\n",
    "AIF360 requires the sensitive attribute to be in the same dataframe (or 2-D array) as the target variable (both the ground truth and the prediction), so we add that here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_aif = pd.concat([X_test_gender['GENDER_M'], y_test], axis=1).set_index('GENDER_M')\n",
    "y_pred_aif = pd.concat([X_test_gender['GENDER_M'].reset_index(drop=True), pd.Series(y_pred_gender)], axis=1).set_index('GENDER_M')\n",
    "y_pred_aif.columns = y_test_aif.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Rates\n",
    "The base rate is the average value of the ground truth (optionally weighted). It provides useful context, although it is not technically a measure of fairness. \n",
    "> $base\\_rate = \\sum_{i=0}^N(y_i)/N$\n",
    "\n",
    "The Selection Rate is the average value of the predicted (ŷ).\n",
    "> $selection\\_rate = \\sum_{i=0}^N(ŷ_i)/N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "base_rate: 0.3801 \n\n          measure     value\n0  selection_rate  0.269314\n"
    }
   ],
   "source": [
    "model_scores =  pd.DataFrame(columns=('measure','value'))\n",
    "print(\"base_rate:\", round(base_rate(y_test_aif, y_pred_aif), 4), \"\\n\")\n",
    "model_scores.loc[0] = ['selection_rate', selection_rate(y_test_aif, y_pred_aif)]\n",
    "print(model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Demographic Parity\n",
    "\n",
    "The Disparate Impact Ratio is the ratio between the probability of positive prediction for the unprivileged group and the probability of positive prediction for the privileged group. A ratio of 1 indicates that the model is fair (it favors neither group).\n",
    "> $disparate\\_impact\\_ratio = P(ŷ =1 | unprivileged) / P(ŷ =1 | privileged)$\n",
    "\n",
    "Statistical Parity Difference is the difference between the selection rate of the privileged group and that of the unprivileged group. A difference of 0 indicates that the model is fair (it favors neither group).\n",
    "> $statistical\\_parity\\_difference = selection\\_rate(ŷ_{unprivileged}) - selection\\_rate(ŷ_{privileged}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                         measure     value\n1         disparate_impact_ratio  1.099128\n2  statistical_parity_difference  0.025555",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>measure</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>disparate_impact_ratio</td>\n      <td>1.099128</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>statistical_parity_difference</td>\n      <td>0.025555</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "model_scores.loc[1] = ['disparate_impact_ratio', disparate_impact_ratio(y_test_aif, y_pred_aif, prot_attr='GENDER_M')]\n",
    "model_scores.loc[2] = ['statistical_parity_difference', statistical_parity_difference(y_test_aif, y_pred_aif, prot_attr='GENDER_M')]\n",
    "model_scores.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Equal Odds\n",
    "Average Odds Difference measures the average of the difference in FPR and TPR for the unprivileged and privileged groups.\n",
    "> $ average\\_odds\\_difference = \\dfrac{(FPR_{unprivileged} - FPR_{privileged})\n",
    "        + (TPR_{unprivileged} - TPR_{privileged})}{2}$\n",
    "\n",
    "Average Odds Error is the average of the absolute difference in FPR and TPR for the unprivileged and privileged groups.\n",
    "> $average\\_odds\\_error = \\dfrac{|FPR_{unprivileged} - FPR_{privileged}|\n",
    "        + |TPR_{unprivileged} - TPR_{privileged}|}{2}$\n",
    "        \n",
    "Equal Opportunity Difference is the difference in recall scores (TPR) between the unprivileged and privileged groups. A difference of 0 indicates that the model is fair.\n",
    "> $equal\\_opportunity\\_difference =  Recall(ŷ_{unprivileged}) - Recall(ŷ_{privileged})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                        measure     value\n3       average_odds_difference  0.013367\n4            average_odds_error  0.015131\n5  equal_opportunity_difference  0.028498",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>measure</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>average_odds_difference</td>\n      <td>0.013367</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>average_odds_error</td>\n      <td>0.015131</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>equal_opportunity_difference</td>\n      <td>0.028498</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "model_scores.loc[3] = ['average_odds_difference', average_odds_difference(y_test_aif, y_pred_aif, prot_attr='GENDER_M')]\n",
    "model_scores.loc[4] = ['average_odds_error', average_odds_error(y_test_aif, y_pred_aif, prot_attr='GENDER_M')]\n",
    "model_scores.loc[5] = ['equal_opportunity_difference', equal_opportunity_difference(y_test_aif, y_pred_aif, prot_attr='GENDER_M')]\n",
    "model_scores.tail(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures Of Individual Fairness\n",
    "Consistency scores measure the similarity between a given prediction and the predictions of \"like\" individuals. In AIF360, the consistency score is calculated as the compliment of the mean distance to the score of the mean nearest neighbhor, using Scikit's Nearest Neighbors algorithm (default 5 neighbors determined by BallTree algorithm).\n",
    "> $ consistency\\_score = 1 - |mean_{distance}(mean({nearest\\ neighbor}) )| $\n",
    "\n",
    "#### The Generalized Entropy Index and Related Measures\n",
    "The Generalized Entropy (GE) Index is...\n",
    "> $ GE =  \\mathcal{E}(\\alpha) = \\begin{cases}\n",
    "            \\frac{1}{n \\alpha (\\alpha-1)}\\sum_{i=1}^n\\left[\\left(\\frac{b_i}{\\mu}\\right)^\\alpha - 1\\right],& \\alpha \\ne 0, 1,\\\\\n",
    "            \\frac{1}{n}\\sum_{i=1}^n\\frac{b_{i}}{\\mu}\\ln\\frac{b_{i}}{\\mu},& \\alpha=1,\\\\\n",
    "            -\\frac{1}{n}\\sum_{i=1}^n\\ln\\frac{b_{i}}{\\mu},& \\alpha=0.\n",
    "        \\end{cases}\n",
    "        $\n",
    "\n",
    "Generalized Entropy Error = Calculates the GE of the set of errors, i.e. 1 + (ŷ == pos_label) - (y == pos_label) \n",
    "> $ GE(Error) = b_i = \\hat{y}_i - y_i + 1 $\n",
    "\n",
    "Between Group Generalized Entropy Error = Calculates the GE of the set of mean errors for the two groups (privileged error & unprivileged error), weighted by the number of predictions in each group\n",
    "> $ GE(Error_{group}) =  GE( [N_{unprivileged}*mean(Error_{unprivileged}), N_{privileged}*mean(Error_{privileged})] ) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                   measure     value\n6                        consistency_score  0.682955\n7                generalized_entropy_error  0.140157\n8  between_group_generalized_entropy_error  0.000015",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>measure</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>consistency_score</td>\n      <td>0.682955</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>generalized_entropy_error</td>\n      <td>0.140157</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>between_group_generalized_entropy_error</td>\n      <td>0.000015</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model_scores.loc[6] = ['consistency_score', consistency_score(X_test_gender, y_pred_gender)]\n",
    "model_scores.loc[7] = ['generalized_entropy_error', generalized_entropy_error(y_test['los_binary'], y_pred_gender)]\n",
    "model_scores.loc[8] = ['between_group_generalized_entropy_error', \n",
    "                            between_group_generalized_entropy_error(y_test_aif, y_pred_aif, prot_attr=['GENDER_M'])]\n",
    "model_scores.tail(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Against a Second Model - Evaluating Unawareness <a class=\"anchor\" id=\"part3\"></a>\n",
    "\n",
    "To demonstrate the change in model scores relative to the use of a sensitive attribute, this section generates a new, though similar model with the sensitive attribute removed. As shown below, for this sensitive attribute, there is no observed difference in scores with the exclusion of the sensitive attribute.\n",
    "\n",
    "Note: Since we have already discussed the individual measures, a helper function will be used to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "base_rate: 0.3801 \n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                   measure  gender_score  \\\n0                           selection_rate        0.2693   \n1                   disparate_impact_ratio        1.0991   \n2            statistical_parity_difference        0.0256   \n3                  average_odds_difference        0.0134   \n4                       average_odds_error        0.0151   \n5             equal_opportunity_difference        0.0285   \n6                        consistency_score        0.6830   \n7                generalized_entropy_error        0.1402   \n8  between_group_generalized_entropy_error        0.0000   \n\n   gender_score (feature removed)  \n0                          0.2693  \n1                          1.0991  \n2                          0.0256  \n3                          0.0134  \n4                          0.0151  \n5                          0.0285  \n6                          0.6830  \n7                          0.3120  \n8                          0.0000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>measure</th>\n      <th>gender_score</th>\n      <th>gender_score (feature removed)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>selection_rate</td>\n      <td>0.2693</td>\n      <td>0.2693</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>disparate_impact_ratio</td>\n      <td>1.0991</td>\n      <td>1.0991</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>statistical_parity_difference</td>\n      <td>0.0256</td>\n      <td>0.0256</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>average_odds_difference</td>\n      <td>0.0134</td>\n      <td>0.0134</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>average_odds_error</td>\n      <td>0.0151</td>\n      <td>0.0151</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>equal_opportunity_difference</td>\n      <td>0.0285</td>\n      <td>0.0285</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>consistency_score</td>\n      <td>0.6830</td>\n      <td>0.6830</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>generalized_entropy_error</td>\n      <td>0.1402</td>\n      <td>0.3120</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>between_group_generalized_entropy_error</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "new_scores = tutorial_helpers.get_aif360_measures_df(X_test_gender, y_test, baseline_y_pred, sensitive_attributes=['GENDER_M'])\n",
    "\n",
    "comparison = model_scores.rename(columns={'value':'gender_score'}\n",
    "                                ).merge(new_scores.rename(columns={'value':'gender_score (feature removed)'}))\n",
    "comparison.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> to do: add peformance functions like roc_auc_score_group_summary from AIF360 to process above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Testing Other Sensitive Attributes\n",
    "\n",
    "Our next experiment will test the presence of bias relative to a patient\\'s language, assuming that there is a bias toward individuals who speak English. As above, we will add a boolean 'LANGUAGE_ENGL' feature to the baseline data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             count    mean     std     min     25%     50%      75%      max\nLANG_ENGL                                                                   \n0          10266.0  9.5874  6.4989  0.0042  4.9115  7.9167  12.8132  29.9792\n1          12168.0  8.7169  5.9239  0.0049  4.4819  7.2174  11.6069  29.9889",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th>LANG_ENGL</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10266.0</td>\n      <td>9.5874</td>\n      <td>6.4989</td>\n      <td>0.0042</td>\n      <td>4.9115</td>\n      <td>7.9167</td>\n      <td>12.8132</td>\n      <td>29.9792</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12168.0</td>\n      <td>8.7169</td>\n      <td>5.9239</td>\n      <td>0.0049</td>\n      <td>4.4819</td>\n      <td>7.2174</td>\n      <td>11.6069</td>\n      <td>29.9889</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Here we attach the sensitive attribute to our data\n",
    "lang_cols = [c for c in df.columns if c.startswith(\"LANGUAGE_\")]\n",
    "eng_cols = ['LANGUAGE_ENGL']\n",
    "X_lang =  df.loc[:,lang_cols]\n",
    "X_lang['LANG_ENGL'] = 0\n",
    "X_lang.loc[X_lang[eng_cols].eq(1).any(axis=1), 'LANG_ENGL'] = 1\n",
    "X_lang = X_lang.drop(lang_cols, axis=1).fillna(0)\n",
    "X_lang.join(df['length_of_stay']).groupby('LANG_ENGL')['length_of_stay'].describe().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n ROC_AUC Score with Gender Included: 0.7227318124596439\n"
    }
   ],
   "source": [
    "# Here we train the model\n",
    "X_lang_train = X_train.join(X_lang, how='inner')\n",
    "X_lang_test = X_test.join(X_lang, how='inner')\n",
    "lang_model = XGBClassifier()\n",
    "lang_model.fit(X_lang_train, y_train)\n",
    "y_pred_lang = lang_model.predict(X_lang_test)\n",
    "print('\\n', \"ROC_AUC Score with Gender Included:\", roc_auc_score(y_test, y_pred_lang) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, by comparing the results with and without the sensitivie attribute we can better demonstrate the effect that the attribute has on the fairness of the model. In this example we see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Measure values with feature included:\nbase_rate: 0.3801 \n\n                         measure   value\n0                 selection_rate  0.2704\n1         disparate_impact_ratio  1.2227\n2  statistical_parity_difference  0.0546\n\n Measure values with feature removed:\nbase_rate: 0.3801 \n\n                         measure   value\n0                 selection_rate  0.2693\n1         disparate_impact_ratio  1.0930\n2  statistical_parity_difference  0.0240\n"
    }
   ],
   "source": [
    "print(\"Measure values with feature included:\")\n",
    "lang_scores = tutorial_helpers.get_aif360_measures_df(X_lang_test, y_test, y_pred_lang, sensitive_attributes=['LANG_ENGL'])\n",
    "print(lang_scores.round(4).head(3))\n",
    "print(\"\\n\", \"Measure values with feature removed:\")\n",
    "lang_ko_scores = tutorial_helpers.get_aif360_measures_df(X_lang_test, y_test, baseline_y_pred, sensitive_attributes=['LANG_ENGL']) \n",
    "print(lang_ko_scores.round(4).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing All Four Models Against Each Other\n",
    "As shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                   measure  gender_score  \\\n0                           selection_rate        0.2693   \n1                   disparate_impact_ratio        1.0991   \n2            statistical_parity_difference        0.0256   \n3                  average_odds_difference        0.0134   \n4                       average_odds_error        0.0151   \n5             equal_opportunity_difference        0.0285   \n6                        consistency_score        0.6830   \n7                generalized_entropy_error        0.1402   \n8  between_group_generalized_entropy_error        0.0000   \n\n   gender_score (feature removed)  lang_score  lang_score (feature removed)  \n0                          0.2693      0.2704                        0.2693  \n1                          1.0991      1.2227                        1.0930  \n2                          0.0256      0.0546                        0.0240  \n3                          0.0134      0.0382                        0.0047  \n4                          0.0151      0.0382                        0.0047  \n5                          0.0285      0.0516                        0.0061  \n6                          0.6830      0.6815                        0.6828  \n7                          0.3120      0.3089                        0.3120  \n8                          0.0000      0.0000                        0.0001  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>measure</th>\n      <th>gender_score</th>\n      <th>gender_score (feature removed)</th>\n      <th>lang_score</th>\n      <th>lang_score (feature removed)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>selection_rate</td>\n      <td>0.2693</td>\n      <td>0.2693</td>\n      <td>0.2704</td>\n      <td>0.2693</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>disparate_impact_ratio</td>\n      <td>1.0991</td>\n      <td>1.0991</td>\n      <td>1.2227</td>\n      <td>1.0930</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>statistical_parity_difference</td>\n      <td>0.0256</td>\n      <td>0.0256</td>\n      <td>0.0546</td>\n      <td>0.0240</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>average_odds_difference</td>\n      <td>0.0134</td>\n      <td>0.0134</td>\n      <td>0.0382</td>\n      <td>0.0047</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>average_odds_error</td>\n      <td>0.0151</td>\n      <td>0.0151</td>\n      <td>0.0382</td>\n      <td>0.0047</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>equal_opportunity_difference</td>\n      <td>0.0285</td>\n      <td>0.0285</td>\n      <td>0.0516</td>\n      <td>0.0061</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>consistency_score</td>\n      <td>0.6830</td>\n      <td>0.6830</td>\n      <td>0.6815</td>\n      <td>0.6828</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>generalized_entropy_error</td>\n      <td>0.1402</td>\n      <td>0.3120</td>\n      <td>0.3089</td>\n      <td>0.3120</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>between_group_generalized_entropy_error</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0001</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "full_comparison = comparison.merge(lang_scores.rename(columns={'value':'lang_score'})\n",
    "                            ).merge(lang_ko_scores.rename(columns={'value':'lang_score (feature removed)'})\n",
    "                            )\n",
    "full_comparison.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparison to FairLearn <a class=\"anchor\" id=\"part5\"></a>\n",
    "\n",
    "Next, some of the same metrics will be demonstrated using Microsoft's FairLearn library. Although both APIs are similar and the measures built into FairLearn are not as comprehensive as those of AIF360, some users may find FairLearn's documentation style to be more accessible. A table comparing the measures available in each library is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<img src=\"img/library_measure_comparison.png\" width=\"500\"/>",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "Image(url=\"img/library_measure_comparison.png\", width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Selection rate 0.27039438141545113\nDemographic parity difference 0.054553219850345586\nDemographic parity ratio 0.8178916255964723\n------\nOverall AUC 0.8277471574588929\nAUC difference {'overall': 0.8277471574588929, 'by_group': {0: 0.8251191163857235, 1: 0.8284959998227068}}\n"
    }
   ],
   "source": [
    "print(\"Selection rate\", \n",
    "      selection_rate(y_test, y_pred_lang) )\n",
    "print(\"Demographic parity difference\", \n",
    "      demographic_parity_difference(y_test, y_pred_lang, sensitive_features=X_lang_test['LANG_ENGL']))\n",
    "print(\"Demographic parity ratio\", \n",
    "      demographic_parity_ratio(y_test, y_pred_lang, sensitive_features=X_lang_test['LANG_ENGL']))\n",
    "\n",
    "print(\"------\")\n",
    "y_prob_lang = lang_model.predict_proba(X_lang_test)[:, 1]\n",
    "print(\"Overall AUC\", roc_auc_score(y_test, y_prob_lang) )\n",
    "print(\"AUC difference\", roc_auc_score_group_summary(y_test, y_prob_lang, sensitive_features=X_lang_test['LANG_ENGL']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Error Rate Difference\n",
    "Similar to the Equal Opportunity Difference measured by AIF360, the Balanced Error Rate Difference offered by FairLearn calculates the difference in accuracy score between the unprivileged and privileged group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-----\nBalanced error rate difference {'overall': 0.7227318124596438, 'by_group': {0: 0.7289100106775893, 1: 0.7155632936639851}}\nEqualized odds difference 0.05159445477325997\n"
    }
   ],
   "source": [
    "\n",
    "print(\"-----\")\n",
    "print(\"Balanced error rate difference\",\n",
    "        balanced_accuracy_score_group_summary(y_test, y_pred_lang, sensitive_features=X_lang_test['LANG_ENGL']))\n",
    "print(\"Equalized odds difference\",\n",
    "      equalized_odds_difference(y_test, y_pred_lang, sensitive_features=X_lang_test['LANG_ENGL']))\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This tutorial introduced multiple measures of ML fairness in the context of a healthcare model using the AIF360 and FairLearn Python libraries. A subset of the MIMIC-III database was used to generate a series of simple Length of Stay (LOS) models. It was shown that while the inclusion of a sensitive feature can significantly affect a model's bias as it relates to that feature, this is not always the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "MIMIC-III, a freely accessible critical care database. Johnson AEW, Pollard TJ, Shen L, Lehman L, Feng M, Ghassemi M, Moody B, Szolovits P, Celi LA, and Mark RG. Scientific Data (2016). DOI: 10.1038/sdata.2016.35. Available from: http://www.nature.com/articles/sdata201635\n",
    "\n",
    "<a id=\"hcup\"></a>\n",
    "HCUP https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<img src=\"library_algorithm_comparison.png\" width=\"500\"/>",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "Image(url=\"library_algorithm_comparison.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CELLS BELOW (TO BE REMOVED BEFORE TUTORIAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIF360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Caucasian Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ETHNICITY_WHITE',\n",
       " 'ETHNICITY_WHITE - BRAZILIAN',\n",
       " 'ETHNICITY_WHITE - EASTERN EUROPEAN',\n",
       " 'ETHNICITY_WHITE - OTHER EUROPEAN',\n",
       " 'ETHNICITY_WHITE - RUSSIAN']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_cols = [c for c in df.columns if c.startswith(\"ETHNICITY_\")]\n",
    "cauc_cols = [c for c in df.columns if c.startswith(\"ETHNICITY_WHITE\")]\n",
    "cauc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    23485.000000\n",
       "mean         0.741750\n",
       "std          0.437681\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          1.000000\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "Name: caucasian, dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_eth =  df.loc[:,eth_cols]\n",
    "X_eth['caucasian'] = 0\n",
    "X_eth.loc[X_eth[cauc_cols].eq(1).any(axis=1), 'caucasian'] = 1\n",
    "X_eth = X_eth.drop(eth_cols, axis=1).fillna(0)\n",
    "X_eth['caucasian'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caucasian</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6065.0</td>\n",
       "      <td>10.811345</td>\n",
       "      <td>10.279624</td>\n",
       "      <td>-0.797222</td>\n",
       "      <td>4.834722</td>\n",
       "      <td>7.899306</td>\n",
       "      <td>13.631250</td>\n",
       "      <td>123.127778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17420.0</td>\n",
       "      <td>10.547957</td>\n",
       "      <td>9.946768</td>\n",
       "      <td>-0.768750</td>\n",
       "      <td>4.844965</td>\n",
       "      <td>7.831944</td>\n",
       "      <td>12.948264</td>\n",
       "      <td>191.422917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count       mean        std       min       25%       50%  \\\n",
       "caucasian                                                                \n",
       "0           6065.0  10.811345  10.279624 -0.797222  4.834722  7.899306   \n",
       "1          17420.0  10.547957   9.946768 -0.768750  4.844965  7.831944   \n",
       "\n",
       "                 75%         max  \n",
       "caucasian                         \n",
       "0          13.631250  123.127778  \n",
       "1          12.948264  191.422917  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_eth.join(df['length_of_stay']).groupby('caucasian')['length_of_stay'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15734, 506) (15734, 507) (7751, 506) (7751, 507)\n"
     ]
    }
   ],
   "source": [
    "X_eth_train = X_train.join(X_eth, how='inner')\n",
    "X_eth_test = X_test.join(X_eth, how='inner')\n",
    "print(X_train.shape, X_eth_train.shape, X_test.shape, X_eth_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christineallen/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/christineallen/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "eth_model = XGBClassifier()\n",
    "eth_model.fit(X_eth_train, y_train)\n",
    "eth_y_pred = eth_model.predict(X_eth_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_rate: 0.3465 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selection_rate</td>\n",
       "      <td>0.2344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disparate_impact_ratio</td>\n",
       "      <td>1.0125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>statistical_parity_difference</td>\n",
       "      <td>0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>average_odds_difference</td>\n",
       "      <td>-0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>average_odds_error</td>\n",
       "      <td>0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>equal_opportunity_difference</td>\n",
       "      <td>-0.0013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>generalized_entropy_error</td>\n",
       "      <td>0.2680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>between_group_generalized_entropy_error</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>consistency_score</td>\n",
       "      <td>0.7122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   measure   value\n",
       "0                           selection_rate  0.2344\n",
       "1                   disparate_impact_ratio  1.0125\n",
       "2            statistical_parity_difference  0.0029\n",
       "3                  average_odds_difference -0.0006\n",
       "4                       average_odds_error  0.0007\n",
       "5             equal_opportunity_difference -0.0013\n",
       "6                generalized_entropy_error  0.2680\n",
       "7  between_group_generalized_entropy_error  0.0000\n",
       "8                        consistency_score  0.7122"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tutorial_helpers.get_aif360_measures_df(X_eth_test, y_test, y_pred, sensitive_attributes=['caucasian']) ).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_rate: 0.3465 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selection_rate</td>\n",
       "      <td>0.2344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disparate_impact_ratio</td>\n",
       "      <td>1.0125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>statistical_parity_difference</td>\n",
       "      <td>0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>average_odds_difference</td>\n",
       "      <td>-0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>average_odds_error</td>\n",
       "      <td>0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>equal_opportunity_difference</td>\n",
       "      <td>-0.0013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>generalized_entropy_error</td>\n",
       "      <td>0.2680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>between_group_generalized_entropy_error</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>consistency_score</td>\n",
       "      <td>0.7122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   measure   value\n",
       "0                           selection_rate  0.2344\n",
       "1                   disparate_impact_ratio  1.0125\n",
       "2            statistical_parity_difference  0.0029\n",
       "3                  average_odds_difference -0.0006\n",
       "4                       average_odds_error  0.0007\n",
       "5             equal_opportunity_difference -0.0013\n",
       "6                generalized_entropy_error  0.2680\n",
       "7  between_group_generalized_entropy_error  0.0000\n",
       "8                        consistency_score  0.7122"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_scores = tutorial_helpers.get_aif360_measures_df(X_eth_test, y_test, eth_y_pred, sensitive_attributes=['caucasian'])\n",
    "eth_scores.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}