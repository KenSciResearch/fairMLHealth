{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Fairness Assessment Template\n",
    "\n",
    "Use this template as a skeleton for comparing fairness and performance measures across a set of trained binary classification models. For an example with a completed comparison, see [tutorials_and_examples/Example-Template-BinaryClassificationAssessment.ipynb](../tutorials_and_examples/Example-Template-BinaryClassificationAssessment.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended list of libraries (optional unless otherwise specified)\n",
    "from fairmlhealth import model_comparison as fhmc, reports, stratified_reports # Required\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Load (or Generate) Data and Models\n",
    "\n",
    "Here you should load (or generate) your test dataset and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < Optional Loading/Cleaning/Training Code Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Evaluate a Single Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Required Variables  \n",
    "\n",
    "* X (NumPy array or similar pandas object): test data to be passed to the models to generate predictions. It's recommended that these be separate data from those used to train the model.\n",
    "\n",
    "* y (NumPy array or similar pandas object): target data array corresponding to X. It is recommended that the target is not present in the test_data.\n",
    "\n",
    "* protected_attr (NumPy array or similar pandas object): protected attributes corresponding to X, optionally also included in X. Note that values must currently be binary- or Boolean-type.\n",
    "\n",
    "* model: the trained models to be evaluated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pointers to be Passed to the Comparison Tools\n",
    "X = None # <- add your test data \n",
    "y = None # <- add your test labels \n",
    "prtc_attr_names = None # list of strings \n",
    "prtc_attr_data = None # if the protected attribute(s) is not present in the data, \n",
    "model = None # add a dict or a list of trained, scikit-compatible models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhmc.measure_model(X, y, prtc_attr_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stratified reports, the protected attribute data must be attached to X\n",
    "if not all(n in list(X) for n in prtc_attr_names):\n",
    "    sr_X = pd.concat([X, protected_attr_data], axis=1)\n",
    "    sr_X = sr_X.loc[ :, set(sr_X.columns.tolist())]\n",
    "else:\n",
    "    sr_X = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_reports.classification_performance(sr_X, y, model.predict(X), features=prtc_attr_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_reports.classification_fairness(sr_X, y, model.predict(X), features=prtc_attr_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Compare Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Required Variables  \n",
    "\n",
    "* X (NumPy array or similar pandas object): test data to be passed to the models to generate predictions. It's recommended that these be separate data from those used to train the model.\n",
    "\n",
    "* y (NumPy array or similar pandas object): target data array corresponding to X. It is recommended that the target is not present in the test_data.\n",
    "\n",
    "* models (list or dict-like): the set of trained models to be evaluated. Note that the dictionary keys are assumed as model names. If a list-like object is passed, the function will set model names relative to their index (i.e. \"model_0\", \"model_1\", etc.)\n",
    "\n",
    "* protected_attr (NumPy array or similar pandas object): protected attributes corresponding to X, optionally also included in X. Note that values must currently be binary- or Boolean-type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pointers to be Passed to the Comparison Tools\n",
    "X = None # <- add your test data (or list/dict of data for each model if model inputs differ)\n",
    "y = None # <- add your test labels (or list/dict of labels for each model if model inputs differ)\n",
    "protected_attr = None # add your protected attribute data (or list/dict of data for each model if model inputs differ)\n",
    "models = None # add a dict or a list of trained, scikit-compatible models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with the FairMLHealth Tool\n",
    "\n",
    "The FairMLHealth model comparison tool generates a table of fairness measures that can be used to quickly compare the fairness-performance tradeoff for a set of fairness-aware models. Comparisons can be called in one of two ways: through an object-oriented method, or through a wrapper function. The section below uses the wrapper function by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhmc.compare_models(test_data = X, \n",
    "                    target_data = y, \n",
    "                    protected_attr_data = protected_attr, \n",
    "                    models = models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with the Fairlearn Dashboard\n",
    "\n",
    "Fairlearn comes with its own model comparison dashboard to allow visual comparison between models.  Note that for binary classification models the ground truth must be passed as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "\n",
    "# FairLearnDashboard Note: for binary classification models, arrays must be passed as a list\n",
    "FairlearnDashboard(sensitive_features=protected_attr.to_list(), \n",
    "                   sensitive_feature_names=['LANGUAGE_ENGL'],\n",
    "                   y_true=y.iloc[:,0].to_list(),\n",
    "                   y_pred={k:model.predict(y) for k,model in models.items()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
