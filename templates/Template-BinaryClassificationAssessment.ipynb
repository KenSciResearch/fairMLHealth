{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Fairness Assessment Template\n",
    "\n",
    "Use this template as a skeleton for comparing fairness and performance measures across a set of trained binary classification models. For an example with a completed comparison, see [tutorials_and_examples/Example-Template-BinaryClassificationAssessment.ipynb](../tutorials_and_examples/Example-Template-BinaryClassificationAssessment.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended list of libraries (optional unless otherwise specified)\n",
    "from fairmlhealth import model_comparison as fhmc, reports, stratified_reports # Required\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Load (or Generate) Data and Models\n",
    "\n",
    "Here you should load (or generate) your test dataset and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < Optional Loading/Cleaning/Training Code Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Evaluate a Single (Baseline) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Required Variables  \n",
    "\n",
    "* X (NumPy array or similar pandas object): test data to be passed to the models to generate predictions. It's recommended that these be separate data from those used to train the model.\n",
    "\n",
    "* y (NumPy array or similar pandas object): target data array corresponding to X. It is recommended that the target is not present in the test data.\n",
    "\n",
    "* PA (NumPy array or similar pandas object): protected attributes corresponding to X, optionally also included in X. Note that values must currently be binary- or Boolean-type.\n",
    "\n",
    "* model: the trained model to be evaluated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pointers to be Passed to the Comparison Tools\n",
    "X = None # <- add your test data \n",
    "y = None # <- add your test labels \n",
    "PA = None # if the protected attribute(s) is not present in the data, \n",
    "model = None # add a dict or a list of trained, scikit-compatible models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhmc.measure_model(PA, y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_reports.data_report(PA, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_reports.classification_performance(PA, y, model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_reports.classification_fairness(PA, y, model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Compare Several Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Required Variables  \n",
    "\n",
    "* X_data (NumPy array or similar pandas object): Test data to be passed to the models to generate predictions (or list/dict of data for each model if model inputs differ). It's recommended that these be separate data from those used to train the model.\n",
    "\n",
    "* y_data (NumPy array or similar pandas object): Target data array corresponding to X (or list/dict of labels for each model if labels differ). It is recommended that the target is not present in the test data.\n",
    "\n",
    "* pa_data (NumPy array or similar pandas object): Protected attributes corresponding to X, optionally also included in X (or list/dict of data for each model if attributes differ). Note that values must currently be binary- or Boolean-type.\n",
    "\n",
    "* models (list or dict-like): The set of trained models to be evaluated (list or dict). Note that the dictionary keys are assumed as model names. If a list-like object is passed, the function will set model names relative to their index (i.e. \"model_0\", \"model_1\", etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pointers to be Passed to the Comparison Tools\n",
    "X_data = [None, None] # <- add your test data (or list/dict of data for each model if model inputs differ)\n",
    "y_data = None # <- add your test labels (or list/dict of labels for each model if labels differ)\n",
    "pa_data = None # add your protected attribute data (or list/dict of data for each model if attributes differ)\n",
    "models = {'model1': None, 'model2': None} # add a dict or a list of trained, scikit-compatible models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with the FairMLHealth Tool\n",
    "\n",
    "The FairMLHealth model comparison tool generates a table of fairness measures that can be used to quickly compare the fairness-performance tradeoff for a set of fairness-aware models. Comparisons can be called in one of two ways: through an object-oriented method, or through a wrapper function. The section below uses the wrapper function by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhmc.compare_models(test_data = X_data, \n",
    "                    targets = y_data, \n",
    "                    protected_attr = pa_data, \n",
    "                    models = models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
