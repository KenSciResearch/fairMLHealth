{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Fairness Assessment Template\n",
    "\n",
    "Use this template as a skeleton for comparing fairness and performance measures across a set of trained regression models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended list of libraries (optional unless otherwise specified)\n",
    "from fairmlhealth import report, measure # Required\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Load (or Generate) Data and Models\n",
    "\n",
    "Here you should load (or generate) your test dataset and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < Optional Loading/Cleaning/Training Code Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Evaluate a Single (Baseline) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Required Variables  \n",
    "\n",
    "* X (NumPy array or similar pandas object): test data to be passed to the models to generate predictions. It's recommended that these be separate data from those used to train the model.\n",
    "\n",
    "* y (NumPy array or similar pandas object): target data array corresponding to X. It is recommended that the target is not present in the test data.\n",
    "\n",
    "* PA (NumPy array or similar pandas object): protected attributes corresponding to X, optionally also included in X. Note that values must currently be binary- or Boolean-type.\n",
    "\n",
    "* model: the trained model to be evaluated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pointers to be Passed to the Comparison Tools\n",
    "X = None # <- add your test data \n",
    "y = None # <- add your test labels \n",
    "PA = None # if the protected attribute(s) is not present in the data, \n",
    "model = None # add a dict or a list of trained, scikit-compatible models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.compare(X, y, PA, model, pred_type = \"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure.data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure.performance(X, y, model.predict(X), pred_type = \"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure.bias(X, y, model.predict(X), pred_type = \"regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
