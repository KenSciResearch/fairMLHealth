{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Fairness Assessment Template\n",
    "\n",
    "Use this template as a skeleton for comparing fairness and performance measures across a set of trained regression models. For an example with a completed comparison, see [examples_and_tutorials/Example-ToolUsage_Regression.ipynb](../examples_and_tutorials/Example-ToolUsage_Regression.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended list of libraries (optional unless otherwise specified)\n",
    "from fairmlhealth import report, measure # Required\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Load (or Generate) Data and Models\n",
    "\n",
    "Here you should load (or generate) your test dataset and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < Optional Loading/Cleaning/Training Code Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Evaluate a Single (Baseline) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Required Variables  \n",
    "\n",
    "* X (NumPy array or similar pandas object): test data to be passed to the models to generate predictions. It's recommended that these be separate data from those used to train the model.\n",
    "\n",
    "* y (NumPy array or similar pandas object): target data array corresponding to X. It is recommended that the target is not present in the test data.\n",
    "\n",
    "* PA (NumPy array or similar pandas object): protected attributes corresponding to X, optionally also included in X. Note that values must currently be binary- or Boolean-type.\n",
    "\n",
    "* model: the trained model to be evaluated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pointers to be Passed to the Comparison Tools\n",
    "X = None # <- add your test data \n",
    "y = None # <- add your test labels \n",
    "PA = None # if the protected attribute(s) is not present in the data, \n",
    "model = None # add a dict or a list of trained, scikit-compatible models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.measure_model(PA, y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure.data(PA, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure.performance(PA, y, model.predict(X), pred_type = \"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure.bias(PA, y, model.predict(X), pred_type = \"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Compare Several Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Required Variables  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pointers to be Passed to the Comparison Tools\n",
    "X_data = [None, None] # <- add your test data (or list/dict of data for each model if model inputs differ)\n",
    "y_data = None # <- add your test labels (or list/dict of labels for each model if labels differ)\n",
    "pa_data = None # add your protected attribute data (or list/dict of data for each model if attributes differ)\n",
    "models = {'model1': None, 'model2': None} # add a dict or a list of trained, scikit-compatible models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with the FairMLHealth Tool\n",
    "\n",
    "The FairMLHealth model comparison tool generates a table of fairness measures that can be used to quickly compare the fairness-performance tradeoff for a set of fairness-aware models. Comparisons can be called in one of two ways: through a class method, or through a wrapper function. The section below uses the wrapper function by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.compare_models(test_data = X_data, \n",
    "                    targets = y_data, \n",
    "                    protected_attr = pa_data, \n",
    "                    models = models,\n",
    "                    pred_type = \"regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
